{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T12:48:27.827141Z",
     "start_time": "2019-02-28T12:48:27.789275Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/entity_education.tsv\", header=None)\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:32:33.409646Z",
     "start_time": "2019-03-01T17:32:33.405658Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import wikipedia\n",
    "import requests\n",
    "from threading import Lock\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:32:35.689102Z",
     "start_time": "2019-03-01T17:32:35.635126Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load links/utils.py\n",
    "QUERY_DICT = {'Organization Founded By^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          ?item wdt:P112 wd:%s.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                           ],\n",
    "              'Organization Founded By':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P112 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                        ],\n",
    "              'Organization Headquarters':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P159 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                          ],\n",
    "              'Organization Subsidiary Of^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P355 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                              ],\n",
    "              'Organization Subsidiary Of':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          ?item wdt:P355 wd:%s.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                           ],\n",
    "              'Organization top employees':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P169 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\", # CEO\n",
    "                                            \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P488 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\" # Chairperson\n",
    "                                            ],\n",
    "              'Person Employee or Member of^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          ?item wdt:P108 wd:%s.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\",\n",
    "                                            \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P527 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\" ## has Part ---> Band Members\n",
    "                                                ],\n",
    "              'Person Employee or Member of':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P108 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                            ],\n",
    "              'Person Place of Birth':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P19 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                      ],\n",
    "              'Person Current and Past Location of Residence':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P551 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                                              ],\n",
    "              'Person Parents':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P22 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\", #Father\n",
    "                                \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P25 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\", #Mother\n",
    "                                \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P1038 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\" #Relative (Adopted Parents?)\n",
    "                                # Shall we include stepparents??\n",
    "                               ],\n",
    "              'Person Parents^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P40 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                  ],\n",
    "              'Person Siblings':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P3373 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                ],\n",
    "              'Person Spouse':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P26 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                              ],\n",
    "              'Citizen of':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P27 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                           ],\n",
    "              'Educated at':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P69 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                            ]\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON   \n",
    "from rosette.api import API, DocumentParameters, RosetteException\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "import requests\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from threading import Lock\n",
    "import os, sys\n",
    "import threading\n",
    "from threading import Thread\n",
    "import time\n",
    "import queue\n",
    "\n",
    "class Utils:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.id_dict = {}\n",
    "        self.lock = Lock()\n",
    "        self.load_dict()\n",
    "        \n",
    "    \n",
    "    def __del__(self):\n",
    "        self.save_dict()\n",
    "\n",
    "    def get_id(self, message, dict_to_use=None):\n",
    "#         if dict_to_use:\n",
    "#             dict_to_use = dict_to_use\n",
    "#         else:\n",
    "#             global id_dict\n",
    "#             dict_to_use = id_dict\n",
    "    \n",
    "        if message in self.id_dict:\n",
    "            return self.id_dict[message]\n",
    "        else:\n",
    "            API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "            query = message\n",
    "            params = {\n",
    "                'action': 'wbsearchentities',\n",
    "                'format': 'json',\n",
    "                'language': 'en',\n",
    "                'search': query\n",
    "            }\n",
    "            r = requests.get(API_ENDPOINT, params = params)\n",
    "            try:\n",
    "                with self.lock:\n",
    "                    self.id_dict[message] = r.json()['search'][0]['id']\n",
    "                return self.id_dict[message]\n",
    "            except Exception:\n",
    "                return -1 #The id doesn't exist\n",
    "\n",
    "\n",
    "    def id_to_name(self, eid):\n",
    "#         if dict_to_use:\n",
    "#             dict_to_use = dict_to_use\n",
    "#         else:\n",
    "#             global id_dict\n",
    "#             dict_to_use = id_dict\n",
    "\n",
    "        if eid in self.id_dict.values():\n",
    "            return [key for key, value in self.id_dict.items() if value == eid][0]\n",
    "        else:\n",
    "            API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "            query = eid\n",
    "            params = {\n",
    "                'action': 'wbsearchentities',\n",
    "                'format': 'json',\n",
    "                'language': 'en',\n",
    "                'search': query\n",
    "            }\n",
    "            r = requests.get(API_ENDPOINT, params = params)\n",
    "            try:\n",
    "                with self.lock:\n",
    "                    self.id_dict[ r.json()['search'][0]['label'] ] = r.json()['search'][0]['id']\n",
    "                return r.json()['search'][0]['label']\n",
    "            except Exception:\n",
    "                return -1 #The id doesn't exist\n",
    "\n",
    "\n",
    "    def get_results(self, query, value, endpoint_url=\"https://query.wikidata.org/sparql\"):\n",
    "        sparql = SPARQLWrapper(endpoint_url)\n",
    "        sparql.setQuery(query%value)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        return sparql.query().convert()\n",
    "\n",
    "\n",
    "    def ground_truth(self, relation, subject, debug=False):\n",
    "        global QUERY_DICT\n",
    "        results = []\n",
    "        gt = []\n",
    "        try:\n",
    "            results = [self.get_results(query, self.get_id(subject)) for query in QUERY_DICT[relation]]\n",
    "            for result in results:\n",
    "                for r in result[\"results\"][\"bindings\"]:\n",
    "                    gt.append(r['itemLabel']['value'])\n",
    "        except:\n",
    "            if debug:\n",
    "                print (relation, subject)\n",
    "        return gt\n",
    "\n",
    "    def add_ground_truth(self, df, debug=False):\n",
    "        if df.empty:\n",
    "            return df\n",
    "        if debug:\n",
    "            print (df)\n",
    "        df = df.reset_index()\n",
    "        df['Pseudo Ground Truth'] = df.apply(lambda row: self.ground_truth(row['Relationship'], row['Subject']), axis=1)\n",
    "        df['Count_PGT'] = df['Pseudo Ground Truth'].apply(lambda x: len(x))\n",
    "        df = df.set_index(['Subject','Relationship'])\n",
    "        return df\n",
    "\n",
    "    def add_recall_score(self, df):\n",
    "        df['Recall Prediction'] = np.random.randint(0, 100, df.shape[0])/100\n",
    "        return df\n",
    "\n",
    "\n",
    "    def load_dict(self):\n",
    "        try:\n",
    "            with open('data/dumps/id_dict.pkl', 'rb') as fp:\n",
    "                self.id_dict = pickle.load(fp)\n",
    "        except:\n",
    "            print (\"Creating a new Dictionary\")\n",
    "            self.id_dict = {}\n",
    "\n",
    "\n",
    "    def save_dict(self):\n",
    "        with self.lock:\n",
    "            old_dict = self.get_dict()\n",
    "            self.id_dict = {**self.id_dict, **old_dict}\n",
    "            with open('data/dumps/id_dict.pkl', 'wb') as fp:\n",
    "                pickle.dump(self.id_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"Saved\")\n",
    "\n",
    "\n",
    "    def get_dict(self):\n",
    "        di = {}\n",
    "        with open('data/dumps/id_dict.pkl', 'rb') as fp:\n",
    "            di = pickle.load(fp)\n",
    "        return di\n",
    "\n",
    "\n",
    "    def Analyse(self, message, alt_url='https://api.rosette.com/rest/v1/'):\n",
    "        \"\"\" Run the example \"\"\"\n",
    "        # Create an API instance\n",
    "        api = API(user_key=\"89350904c7392a44f0f9019563be727a\", service_url=alt_url)\n",
    "\n",
    "        # Set selected API options.\n",
    "        # For more information on the functionality of these\n",
    "        # and other available options, see Rosette Features & Functions\n",
    "        # https://developer.rosette.com/features-and-functions#morphological-analysis-introduction\n",
    "\n",
    "        # api.set_option('modelType','perceptron') #Valid for Chinese and Japanese only\n",
    "\n",
    "        # Opening the ID Dictionary\n",
    "#         load_dict()\n",
    "        ### Will Close after Analysis of the document is completed\n",
    "\n",
    "        params = DocumentParameters()\n",
    "        relationships_text_data = wikipedia.page(message).content[:20000]\n",
    "        params[\"content\"] = relationships_text_data\n",
    "        rel = []\n",
    "        message_id = self.get_id(message)\n",
    "        message_split = message.split(\" \")\n",
    "        try:\n",
    "            RESULT = api.relationships(params)\n",
    "            #print(RESULT)\n",
    "            for r in RESULT['relationships']:\n",
    "                arg2_split = r['arg2'].split(\" \")\n",
    "                confidence = '?'\n",
    "                if \"confidence\" in r:\n",
    "                    confidence = str(round(r[\"confidence\"],2))\n",
    "                if any(s in arg2_split for s in message_split):\n",
    "                    if self.get_id(r['arg2']) == message_id:\n",
    "                        rel.append({'Relationship':r['predicate']+'^-1', 'Subject':r['arg2'], 'Object':r['arg1'], 'Confidence': confidence})\n",
    "                rel.append({'Relationship':r['predicate'],'Subject':r['arg1'],'Object':r['arg2'], 'Confidence': confidence})\n",
    "\n",
    "            ## Closing the ID Dict\n",
    "            self.save_dict()\n",
    "            ##\n",
    "            return rel, message_id\n",
    "        except RosetteException as exception:\n",
    "            print(exception)\n",
    "\n",
    "\n",
    "class HeatMaps(Thread):\n",
    "    def __init__(self, lock, relation='Educated at', eid=None, name=None, rel_dict={}):\n",
    "        Thread.__init__(self)\n",
    "        self.q1 = queue.Queue()\n",
    "        self.q2 = queue.Queue()\n",
    "        self.u = Utils()\n",
    "        self.lock = lock\n",
    "        self.rel_dict = rel_dict\n",
    "        self.eid = eid\n",
    "        self.message = name\n",
    "        self.error = None\n",
    "        self.relation = relation\n",
    "        self.inverse = True if \"^-1\" in relation else False\n",
    "        if name:\n",
    "            self.eid = self.u.get_id(name)\n",
    "        else:\n",
    "            self.message = self.u.id_to_name(eid)\n",
    "        self.start()\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        if self.eid not in self.rel_dict:\n",
    "            a = Thread(target = self.Analyse, args = ())\n",
    "            b = Thread(target = self.ground_truth, args = ())\n",
    "            a.start()\n",
    "            b.start()\n",
    "            a.join()\n",
    "            b.join()\n",
    "        self.matrix_block()\n",
    "\n",
    "\n",
    "    def Analyse(self):\n",
    "        \"\"\" Run the example \"\"\"\n",
    "        # Create an API instance\n",
    "        api = API(user_key=\"89350904c7392a44f0f9019563be727a\", service_url='https://api.rosette.com/rest/v1/')\n",
    "#         u = Utils()\n",
    "        params = DocumentParameters()\n",
    "        relationships_text_data = []\n",
    "        \n",
    "        try:\n",
    "            relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
    "        except wikipedia.DisambiguationError as e:\n",
    "            for n in e.options:\n",
    "                if self.u.get_id(n) == self.eid:\n",
    "                    self.message = n\n",
    "            relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
    "        except wikipedia.exceptions.PageError as e:\n",
    "            self.error = e\n",
    "            print (e)\n",
    "            \n",
    "        params[\"content\"] = relationships_text_data\n",
    "        rel = []\n",
    "        message_id = self.u.get_id(self.message)\n",
    "        message_split = self.message.split(\" \")\n",
    "        pred_list = []\n",
    "        try:\n",
    "            RESULT = []\n",
    "            with self.lock:\n",
    "                RESULT = api.relationships(params)\n",
    "            \n",
    "            args = ['arg1','arg2']\n",
    "            arg_to_split = 'arg2' if self.inverse else 'arg1'\n",
    "            args.remove(arg_to_split)\n",
    "            other_arg = args[0]\n",
    "            rel_to_compare = self.relation.split(\"^-1\")[0]\n",
    "                \n",
    "            for r in RESULT['relationships']:\n",
    "                if r['predicate'] == rel_to_compare:\n",
    "                    arg_split = r[arg_to_split].split(\" \") # Subject Split \n",
    "                    if any(s in arg_split for s in message_split): # Searching for alias names\n",
    "                        if self.u.get_id(r[arg_to_split]) == message_id:\n",
    "                            pred_list.append(r[other_arg])\n",
    "                            \n",
    "            self.q1.put(set(pred_list))\n",
    "        except RosetteException as exception:\n",
    "            print(exception)\n",
    "            self.q1.put(set(pred_list))\n",
    "\n",
    "\n",
    "    def ground_truth(self):\n",
    "#         u = Utils()\n",
    "        \n",
    "        pgt = set(self.u.ground_truth(self.relation, self.message))\n",
    "        self.q2.put(pgt)\n",
    "    \n",
    "    \n",
    "    def matrix_block(self):\n",
    "        if self.eid in self.rel_dict:\n",
    "            self.pgt = self.rel_dict[self.eid]['PGT']\n",
    "            self.extracted = self.rel_dict[self.eid]['Extracted']\n",
    "            self.contained = self.rel_dict[self.eid]['Contained']\n",
    "        else:\n",
    "            q1 = self.q1.get() # Extracted from API\n",
    "            q2 = self.q2.get() # PGT\n",
    "            #print(self.message, q1)\n",
    "            #print(self.message, q2)\n",
    "            self.pgt = len(q2)\n",
    "            self.extracted = len(q1)\n",
    "            q1 = [self.u.get_id(i) for i in q1]\n",
    "            q2 = [self.u.get_id(i) for i in q2]\n",
    "            #print(self.message, q1)\n",
    "            #print(self.message, q2)\n",
    "            count = 0\n",
    "            for i in q1:\n",
    "                if i in q2:\n",
    "                    count += 1\n",
    "            self.contained = count\n",
    "\n",
    "    def get_values(self):\n",
    "        if self.error:\n",
    "            raise Exception(self.error)\n",
    "        return [self.eid, self.message, self.extracted, self.contained, self.pgt]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:32:36.711415Z",
     "start_time": "2019-03-01T17:32:36.138806Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "with open(b'data/has_member_entity.ser') as fp:\n",
    "    data = fp.read()\n",
    "regex = '([Q][0-9]+)' # Regular Expression for the Entity IDs\n",
    "ids = re.findall(regex, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:32:37.212035Z",
     "start_time": "2019-03-01T17:32:37.207078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:32:37.956577Z",
     "start_time": "2019-03-01T17:32:37.605613Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_rel_dict(relation):\n",
    "    path = 'data/dumps/' + str(relation) + '.pkl'\n",
    "    relation_dict = {}\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            relation_dict = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"Creating a new Dictionary\")\n",
    "        relation_dict = {}\n",
    "    return relation_dict\n",
    "        \n",
    "    \n",
    "def save_rel_dict(relation, rel_dict):\n",
    "    path = 'data/dumps/' + str(relation) + '.pkl'\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(rel_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Done!\")\n",
    "        \n",
    "\n",
    "def rel_dict_to_df(relation):\n",
    "    rel_dict = load_rel_dict(relation)\n",
    "    df = pd.DataFrame.from_dict(rel_dict, orient='index')\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-01T17:32:40.194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new Dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"After Hours\" may refer to: \n",
      "After Hours (film)\n",
      "After Hours (Canadian TV series)\n",
      "After Hours (Singaporean TV series)\n",
      "After Hours (UK TV series)\n",
      "\"After Hours\" (Dawson's Creek)\n",
      "\"After Hours\" (House)\n",
      "\"After Hours\" (The Office)\n",
      "\"After Hours\" (Ugly Betty)\n",
      "The After Hours\n",
      "\"The After Hours\" (1986 The Twilight Zone)\n",
      "Afterhour (band)\n",
      "Afterhours (band)\n",
      "Barbershop Harmony Society\n",
      "After Hours (radio show)\n",
      "Charlie Christian\n",
      "After Hours with Joe Bushkin\n",
      "After Hours (1955 Sarah Vaughan album)\n",
      "After Hours (Thad Jones album)\n",
      "After Hours (1961 Sarah Vaughan album)\n",
      "After Hours (Richard Holmes album)\n",
      "After Hours (Hank Crawford album)\n",
      "After Hours (Little River Band album)\n",
      "After Hours (Live in Paris)\n",
      "After Hours: Forward to Scotland's Past\n",
      "After Hours (Pinetop Perkins album)\n",
      "After Hours (André Previn album)\n",
      "After Hours (Gary Moore album)\n",
      "After Hours (Jeanne Lee and Mal Waldron album)\n",
      "After Hours (John Pizzarelli album)\n",
      "After Hours (Linda Perry album)\n",
      "After Hours (Rahsaan Patterson album)\n",
      "After Hours (Glenn Frey album)\n",
      "After Hours (Timeflies album)\n",
      "After Hours (Glamour of the Kill EP)\n",
      "\"After Hours\" (Avery Parrish song)\n",
      "\"Afterhours\" (deadmau5 song)\n",
      "\"After Hours\" (The Velvet Underground song)\n",
      "\"After Hours\" (We Are Scientists song)\n",
      "The Crystal Method\n",
      "Zodiac\n",
      "The Gathering\n",
      "Afterhour club\n",
      "After Hours Formalwear\n",
      "After Hours Press\n",
      "After-hours trading\n",
      "After Hours (novel)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"After Hours\" may refer to: \n",
      "After Hours (film)\n",
      "After Hours (Canadian TV series)\n",
      "After Hours (Singaporean TV series)\n",
      "After Hours (UK TV series)\n",
      "\"After Hours\" (Dawson's Creek)\n",
      "\"After Hours\" (House)\n",
      "\"After Hours\" (The Office)\n",
      "\"After Hours\" (Ugly Betty)\n",
      "The After Hours\n",
      "\"The After Hours\" (1986 The Twilight Zone)\n",
      "Afterhour (band)\n",
      "Afterhours (band)\n",
      "Barbershop Harmony Society\n",
      "After Hours (radio show)\n",
      "Charlie Christian\n",
      "After Hours with Joe Bushkin\n",
      "After Hours (1955 Sarah Vaughan album)\n",
      "After Hours (Thad Jones album)\n",
      "After Hours (1961 Sarah Vaughan album)\n",
      "After Hours (Richard Holmes album)\n",
      "After Hours (Hank Crawford album)\n",
      "After Hours (Little River Band album)\n",
      "After Hours (Live in Paris)\n",
      "After Hours: Forward to Scotland's Past\n",
      "After Hours (Pinetop Perkins album)\n",
      "After Hours (André Previn album)\n",
      "After Hours (Gary Moore album)\n",
      "After Hours (Jeanne Lee and Mal Waldron album)\n",
      "After Hours (John Pizzarelli album)\n",
      "After Hours (Linda Perry album)\n",
      "After Hours (Rahsaan Patterson album)\n",
      "After Hours (Glenn Frey album)\n",
      "After Hours (Timeflies album)\n",
      "After Hours (Glamour of the Kill EP)\n",
      "\"After Hours\" (Avery Parrish song)\n",
      "\"Afterhours\" (deadmau5 song)\n",
      "\"After Hours\" (The Velvet Underground song)\n",
      "\"After Hours\" (We Are Scientists song)\n",
      "The Crystal Method\n",
      "Zodiac\n",
      "The Gathering\n",
      "Afterhour club\n",
      "After Hours Formalwear\n",
      "After Hours Press\n",
      "After-hours trading\n",
      "After Hours (novel)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-157:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Superbus\" may refer to: \n",
      "Superbus (band)\n",
      "18596 Superbus\n",
      "Lucius Tarquinius Superbus\n",
      "Mount Superbus\n",
      "bus\n",
      "Superbus (company)\n",
      "Superbus.com\n",
      "Superbus (transport)\n",
      "Superba (disambiguation)\n",
      "Superbum\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Superbus\" may refer to: \n",
      "Superbus (band)\n",
      "18596 Superbus\n",
      "Lucius Tarquinius Superbus\n",
      "Mount Superbus\n",
      "bus\n",
      "Superbus (company)\n",
      "Superbus.com\n",
      "Superbus (transport)\n",
      "Superba (disambiguation)\n",
      "Superbum\n",
      "\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-181:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"The Vines\" may refer to: \n",
      "The Vines, Oxford\n",
      "The Vines, Western Australia\n",
      "The Vines (band)\n",
      "The Vines of Mendoza\n",
      "Vine (disambiguation)\n",
      "Grapevine (disambiguation)\n",
      "Vine Street (disambiguation)\n",
      "Vine (surname)\n",
      "Vines (surname)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"The Vines\" may refer to: \n",
      "The Vines, Oxford\n",
      "The Vines, Western Australia\n",
      "The Vines (band)\n",
      "The Vines of Mendoza\n",
      "Vine (disambiguation)\n",
      "Grapevine (disambiguation)\n",
      "Vine Street (disambiguation)\n",
      "Vine (surname)\n",
      "Vines (surname)\n",
      "\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-145:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Extreme\" may refer to: \n",
      "Extreme point\n",
      "Maxima and minima\n",
      "Extremophile\n",
      "Extremes on Earth\n",
      "List of extrasolar planet extremes\n",
      "Extremism\n",
      "Extreme Networks\n",
      "Extreme Records\n",
      "Extreme Associates\n",
      "Xtreme Mod\n",
      "Extreme sport\n",
      "Extreme Sports Channel\n",
      "Los Angeles Xtreme\n",
      "Extreme metal\n",
      "Extreme (band)\n",
      "Extreme (album)\n",
      "Xtreme (group)\n",
      "Xtreme (album)\n",
      "Extremes (album)\n",
      "Agostino Carollo\n",
      "Extreme Sports Channel\n",
      "Extreme (1995 TV series)\n",
      "Extreme (2009 TV series)\n",
      "\"Extreme\" (CSI: Miami)\n",
      "Extremes (novel)\n",
      "Extreme Studios\n",
      "Adam X the X-Treme\n",
      "Sharon Osbourne\n",
      "Chevrolet S-10\n",
      "Extremities (disambiguation)\n",
      "Lunatic fringe (disambiguation)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Extreme\" may refer to: \n",
      "Extreme point\n",
      "Maxima and minima\n",
      "Extremophile\n",
      "Extremes on Earth\n",
      "List of extrasolar planet extremes\n",
      "Extremism\n",
      "Extreme Networks\n",
      "Extreme Records\n",
      "Extreme Associates\n",
      "Xtreme Mod\n",
      "Extreme sport\n",
      "Extreme Sports Channel\n",
      "Los Angeles Xtreme\n",
      "Extreme metal\n",
      "Extreme (band)\n",
      "Extreme (album)\n",
      "Xtreme (group)\n",
      "Xtreme (album)\n",
      "Extremes (album)\n",
      "Agostino Carollo\n",
      "Extreme Sports Channel\n",
      "Extreme (1995 TV series)\n",
      "Extreme (2009 TV series)\n",
      "\"Extreme\" (CSI: Miami)\n",
      "Extremes (novel)\n",
      "Extreme Studios\n",
      "Adam X the X-Treme\n",
      "Sharon Osbourne\n",
      "Chevrolet S-10\n",
      "Extremities (disambiguation)\n",
      "Lunatic fringe (disambiguation)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-220:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Enslaved\" may refer to: \n",
      "Slavery\n",
      "Enslaved (band)\n",
      "Greatest Hits\n",
      "Enslaved (Soulfly album)\n",
      "Enslaved (Steel Attack album)\n",
      "Enslaved: Odyssey to the West\n",
      "Bottom (BDSM)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Enslaved\" may refer to: \n",
      "Slavery\n",
      "Enslaved (band)\n",
      "Greatest Hits\n",
      "Enslaved (Soulfly album)\n",
      "Enslaved (Steel Attack album)\n",
      "Enslaved: Odyssey to the West\n",
      "Bottom (BDSM)\n",
      "\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-217:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"After School\" may refer to: \n",
      "After School (1972 film)\n",
      "After School (2003 film)\n",
      "Afterschool\n",
      "After School (TV series)\n",
      "After School (group)\n",
      "Randy Starr\n",
      "10\n",
      "Donald Bailey\n",
      "Brainstorm\n",
      "Get Home Safely\n",
      "After-school activity\n",
      "After school special\n",
      "After School (app)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"After School\" may refer to: \n",
      "After School (1972 film)\n",
      "After School (2003 film)\n",
      "Afterschool\n",
      "After School (TV series)\n",
      "After School (group)\n",
      "Randy Starr\n",
      "10\n",
      "Donald Bailey\n",
      "Brainstorm\n",
      "Get Home Safely\n",
      "After-school activity\n",
      "After school special\n",
      "After School (app)\n",
      "\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-46:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Immortal\" may refer to: \n",
      "The Immortals (1995 film)\n",
      "The Wisdom of Crocodiles\n",
      "Immortal (2004 film)\n",
      "Immortals (2011 film)\n",
      "Immortal (2015 film)\n",
      "The Immortals (2015 film)\n",
      "Immortal (Highlander)\n",
      "Immortal (professional wrestling)\n",
      "Immortality (Fringe)\n",
      "The Immortal (1970 TV series)\n",
      "The Immortal (2000 TV series)\n",
      "The Immortal (Buffyverse)\n",
      "Imortal\n",
      "\"The Immortals\" (NCIS)\n",
      "\"Immortality\" (CSI)\n",
      "Immortals (Persian Empire)\n",
      "Immortals (Byzantine Empire)\n",
      "Old Guard\n",
      "Immortal (band)\n",
      "The Immortals (band)\n",
      "Immortal Records\n",
      "Immortal Technique\n",
      "Roger Nichols (recording engineer)\n",
      "Immortal (Ann Wilson album)\n",
      "Immortal (Anthem album)\n",
      "Immortal (Beth Hart album)\n",
      "Immortal (Bob Catley album)\n",
      "Immortal (Cynthia Clawson album)\n",
      "Immortal (D'espairsRay album)\n",
      "Immortal (For Today album)\n",
      "Immortal (Immortal EP)\n",
      "Immortal (Michael Jackson album)\n",
      "Immortal (Pyramaze album)\n",
      "Immortal (Sarah Geronimo album)\n",
      "Immortal (Tim Dog album)\n",
      "Immortal?\n",
      "Immortals\n",
      "Immortal (The Crüxshadows EP)\n",
      "Immortal (Immortal EP)\n",
      "\"Immortal\" (J. Cole song)\n",
      "\"Immortal\" (Kid Cudi song)\n",
      "Pure Rock Fury\n",
      "Insomniac's Dream\n",
      "Hide from the Sun\n",
      "In the Night\n",
      "Froot\n",
      "\"Immortals\" (Fall Out Boy song)\n",
      "My Immortal\n",
      "\"The Immortals\"\n",
      "I Feel Immortal\n",
      "\"Immortality\" (Pearl Jam song)\n",
      "\"Immortality\" (Celine Dion song)\n",
      "Lesley Gore\n",
      "Immortal (Buffy novel)\n",
      "Immortal (Image Comics)\n",
      "Immortal (trilogy)\n",
      "Immortals (anthology)\n",
      "Immortality (novel)\n",
      "\"The Immortal\" (short story)\n",
      "The Immortals (Barjavel novel)\n",
      "The Immortals (Hickman novel)\n",
      "The Immortals (The Edge Chronicles)\n",
      "The Immortals (series)\n",
      "The Immortals (books)\n",
      "The Immortals (Gunn novel)\n",
      "Immortality, Inc.\n",
      "Thirty-six Poetry Immortals\n",
      "Immortal (MUD)\n",
      "Immortal: The Invisible War\n",
      "Immortal Game\n",
      "The Immortal (video game)\n",
      "Xian (Taoism)\n",
      "Eight Immortals\n",
      "Immortal (comics)\n",
      "Académie française\n",
      "Immortals\n",
      "J. A. Dennis\n",
      "The Immortals (disambiguation)\n",
      "Immortal Guards (disambiguation)\n",
      "Temple of the Five Immortals (disambiguation)\n",
      "Eight Immortals (disambiguation)\n",
      "Immortel (disambiguation)\n",
      "Immortality in fiction\n",
      "List of people who have claimed to be immortal\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Immortal\" may refer to: \n",
      "The Immortals (1995 film)\n",
      "The Wisdom of Crocodiles\n",
      "Immortal (2004 film)\n",
      "Immortals (2011 film)\n",
      "Immortal (2015 film)\n",
      "The Immortals (2015 film)\n",
      "Immortal (Highlander)\n",
      "Immortal (professional wrestling)\n",
      "Immortality (Fringe)\n",
      "The Immortal (1970 TV series)\n",
      "The Immortal (2000 TV series)\n",
      "The Immortal (Buffyverse)\n",
      "Imortal\n",
      "\"The Immortals\" (NCIS)\n",
      "\"Immortality\" (CSI)\n",
      "Immortals (Persian Empire)\n",
      "Immortals (Byzantine Empire)\n",
      "Old Guard\n",
      "Immortal (band)\n",
      "The Immortals (band)\n",
      "Immortal Records\n",
      "Immortal Technique\n",
      "Roger Nichols (recording engineer)\n",
      "Immortal (Ann Wilson album)\n",
      "Immortal (Anthem album)\n",
      "Immortal (Beth Hart album)\n",
      "Immortal (Bob Catley album)\n",
      "Immortal (Cynthia Clawson album)\n",
      "Immortal (D'espairsRay album)\n",
      "Immortal (For Today album)\n",
      "Immortal (Immortal EP)\n",
      "Immortal (Michael Jackson album)\n",
      "Immortal (Pyramaze album)\n",
      "Immortal (Sarah Geronimo album)\n",
      "Immortal (Tim Dog album)\n",
      "Immortal?\n",
      "Immortals\n",
      "Immortal (The Crüxshadows EP)\n",
      "Immortal (Immortal EP)\n",
      "\"Immortal\" (J. Cole song)\n",
      "\"Immortal\" (Kid Cudi song)\n",
      "Pure Rock Fury\n",
      "Insomniac's Dream\n",
      "Hide from the Sun\n",
      "In the Night\n",
      "Froot\n",
      "\"Immortals\" (Fall Out Boy song)\n",
      "My Immortal\n",
      "\"The Immortals\"\n",
      "I Feel Immortal\n",
      "\"Immortality\" (Pearl Jam song)\n",
      "\"Immortality\" (Celine Dion song)\n",
      "Lesley Gore\n",
      "Immortal (Buffy novel)\n",
      "Immortal (Image Comics)\n",
      "Immortal (trilogy)\n",
      "Immortals (anthology)\n",
      "Immortality (novel)\n",
      "\"The Immortal\" (short story)\n",
      "The Immortals (Barjavel novel)\n",
      "The Immortals (Hickman novel)\n",
      "The Immortals (The Edge Chronicles)\n",
      "The Immortals (series)\n",
      "The Immortals (books)\n",
      "The Immortals (Gunn novel)\n",
      "Immortality, Inc.\n",
      "Thirty-six Poetry Immortals\n",
      "Immortal (MUD)\n",
      "Immortal: The Invisible War\n",
      "Immortal Game\n",
      "The Immortal (video game)\n",
      "Xian (Taoism)\n",
      "Eight Immortals\n",
      "Immortal (comics)\n",
      "Académie française\n",
      "Immortals\n",
      "J. A. Dennis\n",
      "The Immortals (disambiguation)\n",
      "Immortal Guards (disambiguation)\n",
      "Temple of the Five Immortals (disambiguation)\n",
      "Eight Immortals (disambiguation)\n",
      "Immortel (disambiguation)\n",
      "Immortality in fiction\n",
      "List of people who have claimed to be immortal\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-154:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Filter\" may refer to: \n",
      "Filter (chemistry)\n",
      "Filter (aquarium)\n",
      "Filter paper\n",
      "Air filter\n",
      "Oil filter\n",
      "Pneumatic filter\n",
      "Water filter\n",
      "Cigarette filter\n",
      "Fuel filter\n",
      "Filtration (wine)\n",
      "Sieve\n",
      "Optical filter\n",
      "Interference filter\n",
      "Dichroic filter\n",
      "Hydrogen-alpha filter\n",
      "Photographic filter\n",
      "Infrared cut-off filter\n",
      "Chelsea filter\n",
      "Astronomical filter\n",
      "Filter (signal processing)\n",
      "Electronic filter\n",
      "Digital filter\n",
      "Analogue filter\n",
      "Filter (higher-order function)\n",
      "Filter (software)\n",
      "Filter (Unix)\n",
      "Filter (video)\n",
      "Email filtering\n",
      "Content-control software\n",
      "Wordfilter\n",
      "Berkeley Packet Filter\n",
      "DSL filter\n",
      "Helicon Filter\n",
      "Filter (large eddy simulation)\n",
      "Kalman filter\n",
      "Filtration (mathematics)\n",
      "Filter (mathematics)\n",
      "Filtering problem (stochastic processes)\n",
      "Filter (TV series)\n",
      "Filter (magazine)\n",
      "Filter Theatre\n",
      "The Filter\n",
      "Filter (band)\n",
      "Bend\n",
      "Category\n",
      "Affective filter\n",
      "Lane splitting\n",
      "Filter coffee\n",
      "Filter feeder\n",
      "Philtre (disambiguation)\n",
      "Lifter (signal processing)\n",
      "Separation process\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Filter\" may refer to: \n",
      "Filter (chemistry)\n",
      "Filter (aquarium)\n",
      "Filter paper\n",
      "Air filter\n",
      "Oil filter\n",
      "Pneumatic filter\n",
      "Water filter\n",
      "Cigarette filter\n",
      "Fuel filter\n",
      "Filtration (wine)\n",
      "Sieve\n",
      "Optical filter\n",
      "Interference filter\n",
      "Dichroic filter\n",
      "Hydrogen-alpha filter\n",
      "Photographic filter\n",
      "Infrared cut-off filter\n",
      "Chelsea filter\n",
      "Astronomical filter\n",
      "Filter (signal processing)\n",
      "Electronic filter\n",
      "Digital filter\n",
      "Analogue filter\n",
      "Filter (higher-order function)\n",
      "Filter (software)\n",
      "Filter (Unix)\n",
      "Filter (video)\n",
      "Email filtering\n",
      "Content-control software\n",
      "Wordfilter\n",
      "Berkeley Packet Filter\n",
      "DSL filter\n",
      "Helicon Filter\n",
      "Filter (large eddy simulation)\n",
      "Kalman filter\n",
      "Filtration (mathematics)\n",
      "Filter (mathematics)\n",
      "Filtering problem (stochastic processes)\n",
      "Filter (TV series)\n",
      "Filter (magazine)\n",
      "Filter Theatre\n",
      "The Filter\n",
      "Filter (band)\n",
      "Bend\n",
      "Category\n",
      "Affective filter\n",
      "Lane splitting\n",
      "Filter coffee\n",
      "Filter feeder\n",
      "Philtre (disambiguation)\n",
      "Lifter (signal processing)\n",
      "Separation process\n",
      "\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-127:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Yes\" may refer to: \n",
      "yes and no\n",
      "YES Prep Public Schools\n",
      "YES (Your Extraordinary Saturday)\n",
      "Young Eisner Scholars\n",
      "Young Epidemiology Scholars\n",
      "yes (Unix)\n",
      "Philips :YES\n",
      "Yes! Roadster\n",
      "Yasuj Airport\n",
      "OLT Express\n",
      "Yale Entrepreneurial Society\n",
      "YES Snowboards\n",
      "The YES! Association\n",
      "Yes! Youth Movement\n",
      "Young European Socialists\n",
      "Youth Empowerment Scheme\n",
      "Youth Energy Squad (Y.E.S)\n",
      "Youth Entrepreneurship and Sustainability\n",
      "YES (Lithuanian political party)\n",
      "Yes! (Hong Kong magazine)\n",
      "Yes! (U.S. magazine)\n",
      "Yes! (Philippine magazine)\n",
      "Yes (novel)\n",
      "Daniel Bryan\n",
      "Yes (film)\n",
      "yes (Israel)\n",
      "YES Network\n",
      "Yes TV\n",
      "WYEZ\n",
      "Y.E.S. 93.3FM\n",
      "Yes (band)\n",
      "Yes Featuring Jon Anderson, Trevor Rabin, Rick Wakeman\n",
      "Yes (musical)\n",
      "Yes (Yes album)\n",
      "The Yes Album\n",
      "Yes (Alvin Slaughter album)\n",
      "Yes! (Chad Brock album)\n",
      "Yes! (Jason Mraz album)\n",
      "Yes! (k-os album)\n",
      "Yes (Mika Nakashima album)\n",
      "Yes (Morphine album)\n",
      "Yes (Pet Shop Boys album)\n",
      "Julie Fuchs\n",
      "Yes L.A.\n",
      "Amber\n",
      "\"Yes!\" (Chad Brock song)\n",
      "\"Yes\" (Coldplay song)\n",
      "\"Yes\" (LMFAO song)\n",
      "\"Yes\" (McAlmont & Butler song)\n",
      "\"Yes\" (Sam Feldt song)\n",
      "A Wolf In Sheep's Clothing\n",
      "Confident\n",
      "Yes\n",
      "Dirty Dancing film soundtrack\n",
      "Louisa Johnson\n",
      "Beyoncé Knowles\n",
      "Billy Swan\n",
      "Connie Cato\n",
      "Cornell Campbell\n",
      "Dee C. Lee\n",
      "Demi Lovato\n",
      "The Family\n",
      "Grapefruit\n",
      "Jay & The Americans\n",
      "Johnny Sandon\n",
      "Manic Street Preachers\n",
      "Merry Clayton\n",
      "Peppino Di Capri\n",
      "Pet Shop Boys\n",
      "Roy Orbison\n",
      "Schaffer the Darklord\n",
      "Tim Moore\n",
      "All pages beginning with Yes\n",
      "Yesss (disambiguation)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Yes\" may refer to: \n",
      "yes and no\n",
      "YES Prep Public Schools\n",
      "YES (Your Extraordinary Saturday)\n",
      "Young Eisner Scholars\n",
      "Young Epidemiology Scholars\n",
      "yes (Unix)\n",
      "Philips :YES\n",
      "Yes! Roadster\n",
      "Yasuj Airport\n",
      "OLT Express\n",
      "Yale Entrepreneurial Society\n",
      "YES Snowboards\n",
      "The YES! Association\n",
      "Yes! Youth Movement\n",
      "Young European Socialists\n",
      "Youth Empowerment Scheme\n",
      "Youth Energy Squad (Y.E.S)\n",
      "Youth Entrepreneurship and Sustainability\n",
      "YES (Lithuanian political party)\n",
      "Yes! (Hong Kong magazine)\n",
      "Yes! (U.S. magazine)\n",
      "Yes! (Philippine magazine)\n",
      "Yes (novel)\n",
      "Daniel Bryan\n",
      "Yes (film)\n",
      "yes (Israel)\n",
      "YES Network\n",
      "Yes TV\n",
      "WYEZ\n",
      "Y.E.S. 93.3FM\n",
      "Yes (band)\n",
      "Yes Featuring Jon Anderson, Trevor Rabin, Rick Wakeman\n",
      "Yes (musical)\n",
      "Yes (Yes album)\n",
      "The Yes Album\n",
      "Yes (Alvin Slaughter album)\n",
      "Yes! (Chad Brock album)\n",
      "Yes! (Jason Mraz album)\n",
      "Yes! (k-os album)\n",
      "Yes (Mika Nakashima album)\n",
      "Yes (Morphine album)\n",
      "Yes (Pet Shop Boys album)\n",
      "Julie Fuchs\n",
      "Yes L.A.\n",
      "Amber\n",
      "\"Yes!\" (Chad Brock song)\n",
      "\"Yes\" (Coldplay song)\n",
      "\"Yes\" (LMFAO song)\n",
      "\"Yes\" (McAlmont & Butler song)\n",
      "\"Yes\" (Sam Feldt song)\n",
      "A Wolf In Sheep's Clothing\n",
      "Confident\n",
      "Yes\n",
      "Dirty Dancing film soundtrack\n",
      "Louisa Johnson\n",
      "Beyoncé Knowles\n",
      "Billy Swan\n",
      "Connie Cato\n",
      "Cornell Campbell\n",
      "Dee C. Lee\n",
      "Demi Lovato\n",
      "The Family\n",
      "Grapefruit\n",
      "Jay & The Americans\n",
      "Johnny Sandon\n",
      "Manic Street Preachers\n",
      "Merry Clayton\n",
      "Peppino Di Capri\n",
      "Pet Shop Boys\n",
      "Roy Orbison\n",
      "Schaffer the Darklord\n",
      "Tim Moore\n",
      "All pages beginning with Yes\n",
      "Yesss (disambiguation)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-283:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"The Fall\" may refer to: \n",
      "fall of man\n",
      "Autumn\n",
      "The Fall (Camus novel)\n",
      "The Fall (del Toro and Hogan novel)\n",
      "The Fall (Muchamore novel)\n",
      "The Fall (Nix novel)\n",
      "The Fall (Star Trek novels)\n",
      "The Fall (band)\n",
      "The Fall (Gorillaz album)\n",
      "The Fall (Norah Jones album)\n",
      "King James\n",
      "Cesium 137\n",
      "XXXTentacion\n",
      "\"The Fall\" (Ministry song)\n",
      "\"The Fall\" (Brendan James song)\n",
      "Beat Happening\n",
      "Xanadu\n",
      "Listen\n",
      "Dead Son Rising\n",
      "Fire\n",
      "0 + 2 = 1\n",
      "Woman\n",
      "Intensify\n",
      "Echoes of Silence\n",
      "The Fall (1969 film)\n",
      "The Fall (1999 film)\n",
      "The Fall (2006 film)\n",
      "The Fall (2008 film)\n",
      "The Fall (TV series)\n",
      "Deus Ex: The Fall\n",
      "The Fall: Last Days of Gaia\n",
      "The Fall (video game)\n",
      "Fall (disambiguation)\n",
      "Fallen (disambiguation)\n",
      "Falling (disambiguation)\n",
      "Fallout (disambiguation)\n",
      "Falls (disambiguation)\n",
      "Fell (disambiguation)\n",
      "The Falls (disambiguation)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"The Fall\" may refer to: \n",
      "fall of man\n",
      "Autumn\n",
      "The Fall (Camus novel)\n",
      "The Fall (del Toro and Hogan novel)\n",
      "The Fall (Muchamore novel)\n",
      "The Fall (Nix novel)\n",
      "The Fall (Star Trek novels)\n",
      "The Fall (band)\n",
      "The Fall (Gorillaz album)\n",
      "The Fall (Norah Jones album)\n",
      "King James\n",
      "Cesium 137\n",
      "XXXTentacion\n",
      "\"The Fall\" (Ministry song)\n",
      "\"The Fall\" (Brendan James song)\n",
      "Beat Happening\n",
      "Xanadu\n",
      "Listen\n",
      "Dead Son Rising\n",
      "Fire\n",
      "0 + 2 = 1\n",
      "Woman\n",
      "Intensify\n",
      "Echoes of Silence\n",
      "The Fall (1969 film)\n",
      "The Fall (1999 film)\n",
      "The Fall (2006 film)\n",
      "The Fall (2008 film)\n",
      "The Fall (TV series)\n",
      "Deus Ex: The Fall\n",
      "The Fall: Last Days of Gaia\n",
      "The Fall (video game)\n",
      "Fall (disambiguation)\n",
      "Fallen (disambiguation)\n",
      "Falling (disambiguation)\n",
      "Fallout (disambiguation)\n",
      "Falls (disambiguation)\n",
      "Fell (disambiguation)\n",
      "The Falls (disambiguation)\n",
      "\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-292:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Testament\" may refer to: \n",
      "Old Testament\n",
      "New Testament\n",
      "John Romer\n",
      "Le Testament\n",
      "fr:Le Testament (Maupassant)\n",
      "Lenin's Testament\n",
      "Testament phonographe\n",
      "The Testament (Elie Wiesel novel)\n",
      "The Testament (John Grisham novel)\n",
      "The Testament (Van Lustbader novel)\n",
      "David Morrell\n",
      "Nino Ricci\n",
      "Testament (comic book)\n",
      "Milos 'Misa' Radivojevic\n",
      "Testament (1983 film)\n",
      "John Akomfrah\n",
      "Testament (2004 film)\n",
      "65th Bodil Awards\n",
      "John Romer\n",
      "Testament (Guilty Gear)\n",
      "Testament (Xenosaga)\n",
      "Henri Duparc\n",
      "Brett Dean\n",
      "Jonathan Cole\n",
      "Le Testament de Villon (opera)\n",
      "Testament (band)\n",
      "Testament Records (UK)\n",
      "Testament Records (USA)\n",
      "Testament (album)\n",
      "Talley Trio\n",
      "Toroidh\n",
      "The Testament (album)\n",
      "Das Testament (E Nomine album)\n",
      "Léo Ferré\n",
      "Blindsided\n",
      "\"Testament\" (Matt Fishel song)\n",
      "\"Testament\" (Nana Mizuki song)\n",
      "Testimony\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Testament\" may refer to: \n",
      "Old Testament\n",
      "New Testament\n",
      "John Romer\n",
      "Le Testament\n",
      "fr:Le Testament (Maupassant)\n",
      "Lenin's Testament\n",
      "Testament phonographe\n",
      "The Testament (Elie Wiesel novel)\n",
      "The Testament (John Grisham novel)\n",
      "The Testament (Van Lustbader novel)\n",
      "David Morrell\n",
      "Nino Ricci\n",
      "Testament (comic book)\n",
      "Milos 'Misa' Radivojevic\n",
      "Testament (1983 film)\n",
      "John Akomfrah\n",
      "Testament (2004 film)\n",
      "65th Bodil Awards\n",
      "John Romer\n",
      "Testament (Guilty Gear)\n",
      "Testament (Xenosaga)\n",
      "Henri Duparc\n",
      "Brett Dean\n",
      "Jonathan Cole\n",
      "Le Testament de Villon (opera)\n",
      "Testament (band)\n",
      "Testament Records (UK)\n",
      "Testament Records (USA)\n",
      "Testament (album)\n",
      "Talley Trio\n",
      "Toroidh\n",
      "The Testament (album)\n",
      "Das Testament (E Nomine album)\n",
      "Léo Ferré\n",
      "Blindsided\n",
      "\"Testament\" (Matt Fishel song)\n",
      "\"Testament\" (Nana Mizuki song)\n",
      "Testimony\n",
      "\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "Exception in thread Thread-286:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 329, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Survivor\" may refer to: \n",
      "Category:Survivors\n",
      "Last survivors of historical events\n",
      "KKnD video-game series\n",
      "Divine Madness\n",
      "Bima Stagg\n",
      "Cyril Nri\n",
      "Danielle Chuchran\n",
      "Survivor (film)\n",
      "Survivors (film)\n",
      "The Survivors (1983 film)\n",
      "The Survivor (2016 film)\n",
      "Survivor (1982 video game)\n",
      "Survivor (1987 video game)\n",
      "Survivor (2001 video game)\n",
      "Survivors (video game)\n",
      "Resident Evil Survivor\n",
      "Survivor (Octavia Butler novel)\n",
      "Survivor (Gonzalez novel)\n",
      "Survivor (Palahniuk novel)\n",
      "\"Survivor\" (story)\n",
      "Christina Crawford\n",
      "William W. Johnstone\n",
      "Tabitha King\n",
      "James Clancy Phelan\n",
      "Survivors (novel)\n",
      "Survivors\n",
      "Survivors (Star Trek)\n",
      "Terry Nation\n",
      "Survivors: A Novel of the Coming Collapse\n",
      "Survivor (band)\n",
      "Survivor Records\n",
      "Survivor (Randy Bachman 1978 album)\n",
      "Survivor (Eric Burdon album)\n",
      "Survivor (Destiny's Child album)\n",
      "Survivor (Fifteen album)\n",
      "Survivor (George Fox album)\n",
      "Survivor (Survivor album)\n",
      "Survivor (Funker Vogt album)\n",
      "Hillary Hawkins\n",
      "Survivors (Max Roach album)\n",
      "Survivors (Samson album)\n",
      "\"Survivor\" (Destiny's Child song)\n",
      "\"Survivor\" (Elena Paparizou song)\n",
      "\"Survivor\" (TVXQ song)\n",
      "Cindy Bullens\n",
      "Mike Francis\n",
      "Mavado\n",
      "M.I.A.\n",
      "Platinum Underground\n",
      "Zach Williams\n",
      "Selena Gomez\n",
      "Hardwell\n",
      "Live On Forever\n",
      "Zomboy\n",
      "Survivor (franchise)\n",
      "Australian Survivor\n",
      "Survivor (Israel)\n",
      "Survivor (UK TV series)\n",
      "Survivor (U.S. TV series)\n",
      "Survivor BG\n",
      "Survivor Philippines\n",
      "Survivor South Africa\n",
      "Survivor Srbija\n",
      "a list of other international versions\n",
      "Survivor Series\n",
      "Survivors (1975 TV series)\n",
      "Survivors (2008 TV series)\n",
      "\"Survivors\" (Babylon 5)\n",
      "\"Survivors\" (Supergirl)\n",
      "\"The Survivors\" (Star Trek: The Next Generation)\n",
      "Survivor (horse)\n",
      "Survivor: The Ride!\n",
      "Sole Survivor (disambiguation)\n",
      "Survival (disambiguation)\n",
      "The Survivor (disambiguation)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-4b7f3c4ca2ef>\", line 334, in Analyse\n",
      "    relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 276, in page\n",
      "    return WikipediaPage(title, redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 299, in __init__\n",
      "    self.__load(redirect=redirect, preload=preload)\n",
      "  File \"C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py\", line 393, in __load\n",
      "    raise DisambiguationError(getattr(self, 'title', page['title']), may_refer_to)\n",
      "wikipedia.exceptions.DisambiguationError: \"Survivor\" may refer to: \n",
      "Category:Survivors\n",
      "Last survivors of historical events\n",
      "KKnD video-game series\n",
      "Divine Madness\n",
      "Bima Stagg\n",
      "Cyril Nri\n",
      "Danielle Chuchran\n",
      "Survivor (film)\n",
      "Survivors (film)\n",
      "The Survivors (1983 film)\n",
      "The Survivor (2016 film)\n",
      "Survivor (1982 video game)\n",
      "Survivor (1987 video game)\n",
      "Survivor (2001 video game)\n",
      "Survivors (video game)\n",
      "Resident Evil Survivor\n",
      "Survivor (Octavia Butler novel)\n",
      "Survivor (Gonzalez novel)\n",
      "Survivor (Palahniuk novel)\n",
      "\"Survivor\" (story)\n",
      "Christina Crawford\n",
      "William W. Johnstone\n",
      "Tabitha King\n",
      "James Clancy Phelan\n",
      "Survivors (novel)\n",
      "Survivors\n",
      "Survivors (Star Trek)\n",
      "Terry Nation\n",
      "Survivors: A Novel of the Coming Collapse\n",
      "Survivor (band)\n",
      "Survivor Records\n",
      "Survivor (Randy Bachman 1978 album)\n",
      "Survivor (Eric Burdon album)\n",
      "Survivor (Destiny's Child album)\n",
      "Survivor (Fifteen album)\n",
      "Survivor (George Fox album)\n",
      "Survivor (Survivor album)\n",
      "Survivor (Funker Vogt album)\n",
      "Hillary Hawkins\n",
      "Survivors (Max Roach album)\n",
      "Survivors (Samson album)\n",
      "\"Survivor\" (Destiny's Child song)\n",
      "\"Survivor\" (Elena Paparizou song)\n",
      "\"Survivor\" (TVXQ song)\n",
      "Cindy Bullens\n",
      "Mike Francis\n",
      "Mavado\n",
      "M.I.A.\n",
      "Platinum Underground\n",
      "Zach Williams\n",
      "Selena Gomez\n",
      "Hardwell\n",
      "Live On Forever\n",
      "Zomboy\n",
      "Survivor (franchise)\n",
      "Australian Survivor\n",
      "Survivor (Israel)\n",
      "Survivor (UK TV series)\n",
      "Survivor (U.S. TV series)\n",
      "Survivor BG\n",
      "Survivor Philippines\n",
      "Survivor South Africa\n",
      "Survivor Srbija\n",
      "a list of other international versions\n",
      "Survivor Series\n",
      "Survivors (1975 TV series)\n",
      "Survivors (2008 TV series)\n",
      "\"Survivors\" (Babylon 5)\n",
      "\"Survivors\" (Supergirl)\n",
      "\"The Survivors\" (Star Trek: The Next Generation)\n",
      "Survivor (horse)\n",
      "Survivor: The Ride!\n",
      "Sole Survivor (disambiguation)\n",
      "Survival (disambiguation)\n",
      "The Survivor (disambiguation)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix_extr = np.zeros([20,20]) ## The HeatMap for all extracted Relations by Rosette\n",
    "matrix_cont = np.zeros([20,20]) ## The HeatMap for all contained (in PGT) relations\n",
    "lock = Lock() ## Shared Lock\n",
    "n = 100\n",
    "relation = 'Person Employee or Member of^-1' #Child\n",
    "topN_entities = ids[:n]#list(df['EntityID'][:n])\n",
    "u = Utils()\n",
    "\n",
    "### Creating threads for each Entity\n",
    "threads = []\n",
    "rel_dict = load_rel_dict(relation)\n",
    "for i in topN_entities:\n",
    "    threads.append( HeatMaps(lock, eid=i, relation=relation, rel_dict=rel_dict) )\n",
    "\n",
    "### Waiting for each thread to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "\n",
    "### Filling up the HeatMap\n",
    "table = PrettyTable(['Name', 'Extracted', 'PGT', 'Contained'])\n",
    "for i,t in enumerate(threads):\n",
    "    try:\n",
    "        eid, name,r_ext,r_cont,c = t.get_values()\n",
    "        rel_dict[eid] = {'EntityName':name, 'Extracted':r_ext, 'PGT':c, 'Contained':r_cont}\n",
    "        table.add_row([name,r_ext,c, r_cont])\n",
    "        matrix_cont[ r_cont, c] += 1\n",
    "        matrix_extr[ r_ext, c] += 1\n",
    "    except:\n",
    "        print(\"Not saving in Dict\", i, ids[i], u.id_to_name(ids[i]))\n",
    "\n",
    "save_rel_dict(relation, rel_dict)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:22:33.270097Z",
     "start_time": "2019-03-01T17:22:33.257164Z"
    }
   },
   "outputs": [],
   "source": [
    "relation = \"Person Parents^-1\"\n",
    "\n",
    "dd = rel_dict_to_df(relation)\n",
    "dd[dd.PGT != 0]\n",
    "dd[dd.EntityName == \"George H. W. Bush\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:21:12.251449Z",
     "start_time": "2019-03-01T17:21:12.246462Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 20, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:25:25.401827Z",
     "start_time": "2019-03-01T17:25:21.317113Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "# Contained\n",
    "sns.heatmap(matrix_cont, ax=ax1, fmt='.0f', cmap='gist_gray_r', xticklabels = [\"\"], yticklabels = [\"\"], annot = True, cbar_kws={\"orientation\": \"horizontal\"})\n",
    "ax1.set_ylabel('Rosette')    \n",
    "ax1.set_xlabel('PGT')\n",
    "ax1.set_title('Contained(in PGT) Vs. PGT')\n",
    "\n",
    "# Extracted\n",
    "sns.heatmap(matrix_extr, ax=ax2, fmt='.0f', cmap='gist_gray_r', xticklabels = [\"\"], yticklabels = [\"\"], annot = True, cbar_kws={\"orientation\": \"horizontal\"})\n",
    "ax2.set_ylabel('Rosette')    \n",
    "ax2.set_xlabel('PGT')\n",
    "ax2.set_title('Extracted(API) Vs. PGT')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"data/img/{}-{}.png\".format(relation,n))\n",
    "\n",
    "matrix_cont.dump('data/numpy_matrices/{}-{}-matrix_cont.dat'.format(relation,n))\n",
    "matrix_extr.dump('data/numpy_matrices/{}-{}-matrix_extr.dat'.format(relation,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:25:26.226591Z",
     "start_time": "2019-03-01T17:25:25.403792Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig1 = sns.heatmap(matrix_cont, linewidth=0.5)\n",
    "fig2 = sns.heatmap(matrix_extr, linewidth=0.5)\n",
    "\n",
    "fig1.set_title('Contained(in PGT) Vs. PGT')\n",
    "fig2.set_title('Extracted(API) Vs. PGT')\n",
    "fig1.set(xlabel='PGT', ylabel='API')\n",
    "fig2.set(xlabel='PGT', ylabel='API')\n",
    "\n",
    "fig1.get_figure().savefig(\"data/img/{}-{}-matrix_cont.png\".format(relation,n))\n",
    "fig2.get_figure().savefig(\"data/img/{}-{}-matrix_extr.png\".format(relation,n))\n",
    "\n",
    "matrix_cont.dump('data/numpy_matrices/{}-{}-matrix_cont.dat'.format(relation,n))\n",
    "matrix_extr.dump('data/numpy_matrices/{}-{}-matrix_extr.dat'.format(relation,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T11:36:12.580006Z",
     "start_time": "2019-03-01T11:36:11.989100Z"
    }
   },
   "outputs": [],
   "source": [
    "fig1.set(xlabel='PGT', ylabel='API')\n",
    "fig1.set_title('Contained(in PGT) Vs. PGT')\n",
    "fig1.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T11:36:13.164241Z",
     "start_time": "2019-03-01T11:36:12.583265Z"
    }
   },
   "outputs": [],
   "source": [
    "fig2.set(xlabel='PGT', ylabel='API')\n",
    "fig2.set_title('Extracted(API) Vs. PGT')\n",
    "fig2.get_figure()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Child, Educated, Spouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T17:28:52.239362Z",
     "start_time": "2019-03-01T17:28:51.901919Z"
    }
   },
   "outputs": [],
   "source": [
    "u = Utils()\n",
    "u.id_to_name(\"Q484427\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
