{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:18:00.815931Z",
     "start_time": "2019-03-07T15:17:51.244886Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import wikipedia\n",
    "import requests\n",
    "from threading import Lock\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:18:04.850185Z",
     "start_time": "2019-03-07T15:18:04.606516Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load links/utils.py\n",
    "QUERY_DICT = {'Organization Founded By^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          ?item wdt:P112 wd:%s.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                           ],\n",
    "              'Organization Founded By':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P112 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                        ],\n",
    "              'Organization Headquarters':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P159 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                          ],\n",
    "              'Organization Subsidiary Of^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P355 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                              ],\n",
    "              'Organization Subsidiary Of':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          ?item wdt:P355 wd:%s.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                           ],\n",
    "              'Organization top employees':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P169 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\", # CEO\n",
    "                                            \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P488 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\" # Chairperson\n",
    "                                            ],\n",
    "              'Person Employee or Member of^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          ?item wdt:P108 wd:%s.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\",\n",
    "                                            \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P527 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\" \n",
    "                                                ],\n",
    "              'Person Employee or Member of':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P108 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\",\n",
    "                                              \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P463 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"## member of ---> Band Members\n",
    "                                            ],\n",
    "              'Person Place of Birth':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P19 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                      ],\n",
    "              'Person Current and Past Location of Residence':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P551 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                                              ],\n",
    "              'Person Parents':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P22 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\", #Father\n",
    "                                \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P25 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\", #Mother\n",
    "                                \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P1038 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\" #Relative (Adopted Parents?)\n",
    "                                # Shall we include stepparents??\n",
    "                               ],\n",
    "              'Person Parents^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P40 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                  ],\n",
    "              'Person Siblings':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P3373 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                ],\n",
    "              'Person Spouse':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P26 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                              ],\n",
    "              'Citizen of':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P27 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                           ],\n",
    "              'Educated at':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P69 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                            ]\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON   \n",
    "from rosette.api import API, DocumentParameters, RosetteException\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "import requests\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from threading import Lock\n",
    "import os, sys\n",
    "import threading\n",
    "from threading import Thread\n",
    "import time\n",
    "import queue\n",
    "\n",
    "class Utils:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.id_dict = {}\n",
    "        self.lock = Lock()\n",
    "        self.load_dict()\n",
    "        \n",
    "    \n",
    "    def __del__(self):\n",
    "        self.save_dict()\n",
    "\n",
    "    def get_id(self, message, dict_to_use=None):\n",
    "#         if dict_to_use:\n",
    "#             dict_to_use = dict_to_use\n",
    "#         else:\n",
    "#             global id_dict\n",
    "#             dict_to_use = id_dict\n",
    "    \n",
    "        if message in self.id_dict:\n",
    "            return self.id_dict[message]\n",
    "        else:\n",
    "            API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "            query = message\n",
    "            params = {\n",
    "                'action': 'wbsearchentities',\n",
    "                'format': 'json',\n",
    "                'language': 'en',\n",
    "                'search': query\n",
    "            }\n",
    "            r = requests.get(API_ENDPOINT, params = params)\n",
    "            try:\n",
    "                with self.lock:\n",
    "                    self.id_dict[message] = r.json()['search'][0]['id']\n",
    "                return self.id_dict[message]\n",
    "            except Exception:\n",
    "                return -1 #The id doesn't exist\n",
    "\n",
    "\n",
    "    def id_to_name(self, eid):\n",
    "#         if dict_to_use:\n",
    "#             dict_to_use = dict_to_use\n",
    "#         else:\n",
    "#             global id_dict\n",
    "#             dict_to_use = id_dict\n",
    "\n",
    "        if eid in self.id_dict.values():\n",
    "            return [key for key, value in self.id_dict.items() if value == eid][0]\n",
    "        else:\n",
    "            API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "            query = eid\n",
    "            params = {\n",
    "                'action': 'wbsearchentities',\n",
    "                'format': 'json',\n",
    "                'language': 'en',\n",
    "                'search': query\n",
    "            }\n",
    "            r = requests.get(API_ENDPOINT, params = params)\n",
    "            try:\n",
    "                with self.lock:\n",
    "                    self.id_dict[ r.json()['search'][0]['label'] ] = r.json()['search'][0]['id']\n",
    "                return r.json()['search'][0]['label']\n",
    "            except Exception:\n",
    "                return -1 #The id doesn't exist\n",
    "\n",
    "\n",
    "    def get_results(self, query, value, endpoint_url=\"https://query.wikidata.org/sparql\"):\n",
    "        sparql = SPARQLWrapper(endpoint_url)\n",
    "        sparql.setQuery(query%value)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        return sparql.query().convert()\n",
    "\n",
    "\n",
    "    def ground_truth(self, relation, subject, debug=False):\n",
    "        global QUERY_DICT\n",
    "        results = []\n",
    "        gt = []\n",
    "        try:\n",
    "            results = [self.get_results(query, self.get_id(subject)) for query in QUERY_DICT[relation]]\n",
    "            for result in results:\n",
    "                for r in result[\"results\"][\"bindings\"]:\n",
    "                    gt.append(r['itemLabel']['value'])\n",
    "        except:\n",
    "            if debug:\n",
    "                print (relation, subject)\n",
    "        return gt\n",
    "\n",
    "    def add_ground_truth(self, df, debug=False):\n",
    "        if df.empty:\n",
    "            return df\n",
    "        if debug:\n",
    "            print (df)\n",
    "        df = df.reset_index()\n",
    "        df['Pseudo Ground Truth'] = df.apply(lambda row: self.ground_truth(row['Relationship'], row['Subject']), axis=1)\n",
    "        df['Count_PGT'] = df['Pseudo Ground Truth'].apply(lambda x: len(x))\n",
    "        df = df.set_index(['Subject','Relationship'])\n",
    "        return df\n",
    "\n",
    "    def add_recall_score(self, df):\n",
    "        df['Recall Prediction'] = np.random.randint(0, 100, df.shape[0])/100\n",
    "        return df\n",
    "\n",
    "\n",
    "    def load_dict(self):\n",
    "        try:\n",
    "            with open('data/dumps/id_dict.pkl', 'rb') as fp:\n",
    "                self.id_dict = pickle.load(fp)\n",
    "        except:\n",
    "            print (\"Creating a new Dictionary\")\n",
    "            self.id_dict = {}\n",
    "\n",
    "\n",
    "    def save_dict(self):\n",
    "        with self.lock:\n",
    "            old_dict = self.get_dict()\n",
    "            self.id_dict = {**self.id_dict, **old_dict}\n",
    "            with open('data/dumps/id_dict.pkl', 'wb') as fp:\n",
    "                pickle.dump(self.id_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"Saved\")\n",
    "\n",
    "\n",
    "    def get_dict(self):\n",
    "        di = {}\n",
    "        with open('data/dumps/id_dict.pkl', 'rb') as fp:\n",
    "            di = pickle.load(fp)\n",
    "        return di\n",
    "\n",
    "\n",
    "    def Analyse(self, message, alt_url='https://api.rosette.com/rest/v1/'):\n",
    "        \"\"\" Run the example \"\"\"\n",
    "        # Create an API instance\n",
    "        api = API(user_key=\"89350904c7392a44f0f9019563be727a\", service_url=alt_url)\n",
    "\n",
    "        # Set selected API options.\n",
    "        # For more information on the functionality of these\n",
    "        # and other available options, see Rosette Features & Functions\n",
    "        # https://developer.rosette.com/features-and-functions#morphological-analysis-introduction\n",
    "\n",
    "        # api.set_option('modelType','perceptron') #Valid for Chinese and Japanese only\n",
    "\n",
    "        # Opening the ID Dictionary\n",
    "#         load_dict()\n",
    "        ### Will Close after Analysis of the document is completed\n",
    "\n",
    "        params = DocumentParameters()\n",
    "        relationships_text_data = wikipedia.page(message).content[:20000]\n",
    "        params[\"content\"] = relationships_text_data\n",
    "        rel = []\n",
    "        message_id = self.get_id(message)\n",
    "        message_split = message.split(\" \")\n",
    "        try:\n",
    "            RESULT = api.relationships(params)\n",
    "            #print(RESULT)\n",
    "            for r in RESULT['relationships']:\n",
    "                arg2_split = r['arg2'].split(\" \")\n",
    "                confidence = '?'\n",
    "                if \"confidence\" in r:\n",
    "                    confidence = str(round(r[\"confidence\"],2))\n",
    "                if any(s in arg2_split for s in message_split):\n",
    "                    if self.get_id(r['arg2']) == message_id:\n",
    "                        rel.append({'Relationship':r['predicate']+'^-1', 'Subject':r['arg2'], 'Object':r['arg1'], 'Confidence': confidence})\n",
    "                rel.append({'Relationship':r['predicate'],'Subject':r['arg1'],'Object':r['arg2'], 'Confidence': confidence})\n",
    "\n",
    "            ## Closing the ID Dict\n",
    "            self.save_dict()\n",
    "            ##\n",
    "            return rel, message_id\n",
    "        except RosetteException as exception:\n",
    "            print(exception)\n",
    "\n",
    "\n",
    "class HeatMaps(Thread):\n",
    "    def __init__(self, lock, relation='Educated at', eid=None, name=None, rel_dict={}):\n",
    "        Thread.__init__(self)\n",
    "        self.q1 = queue.Queue()\n",
    "        self.q2 = queue.Queue()\n",
    "        self.u = Utils()\n",
    "        self.lock = lock\n",
    "        self.rel_dict = rel_dict\n",
    "        self.eid = eid\n",
    "        self.message = name\n",
    "        self.error = None\n",
    "        self.relation = relation\n",
    "        self.inverse = True if \"^-1\" in relation else False\n",
    "        if name:\n",
    "            self.eid = self.u.get_id(name)\n",
    "        else:\n",
    "            self.message = self.u.id_to_name(eid)\n",
    "        self.start()\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        if self.eid not in self.rel_dict:\n",
    "            a = Thread(target = self.Analyse, args = ())\n",
    "            b = Thread(target = self.ground_truth, args = ())\n",
    "            a.start()\n",
    "            b.start()\n",
    "            a.join()\n",
    "            b.join()\n",
    "        self.matrix_block()\n",
    "\n",
    "\n",
    "    def Analyse(self):\n",
    "        \"\"\" Run the example \"\"\"\n",
    "        # Create an API instance\n",
    "        api = API(user_key=\"89350904c7392a44f0f9019563be727a\", service_url='https://api.rosette.com/rest/v1/')\n",
    "#         u = Utils()\n",
    "        params = DocumentParameters()\n",
    "        relationships_text_data = []\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                relationships_text_data = wikipedia.page(self.message).content[:20000]\n",
    "                break\n",
    "            except wikipedia.DisambiguationError as e:\n",
    "                print(self.eid, self.message)\n",
    "                nameclash = True\n",
    "                for n in e.options:\n",
    "                    if self.u.get_id(n) == self.eid:\n",
    "                        if n == self.message:\n",
    "                            pass\n",
    "                        else:\n",
    "                            self.message = n\n",
    "                            nameclash = False\n",
    "                            break\n",
    "                if nameclash:\n",
    "                    self.message = \" \"\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                self.error = self.u.id_to_name(self.eid) + \" \" + str(e)\n",
    "                print (self.error)\n",
    "                break\n",
    "            \n",
    "        params[\"content\"] = relationships_text_data\n",
    "        rel = []\n",
    "        message_id = self.u.get_id(self.message)\n",
    "        message_split = self.message.split(\" \")\n",
    "        pred_list = []\n",
    "        try:\n",
    "            RESULT = []\n",
    "            with self.lock:\n",
    "                RESULT = api.relationships(params)\n",
    "            \n",
    "            args = ['arg1','arg2']\n",
    "            arg_to_split = 'arg2' if self.inverse else 'arg1'\n",
    "            args.remove(arg_to_split)\n",
    "            other_arg = args[0]\n",
    "            rel_to_compare = self.relation.split(\"^-1\")[0]\n",
    "                \n",
    "            for r in RESULT['relationships']:\n",
    "                if r['predicate'] == rel_to_compare:\n",
    "                    arg_split = r[arg_to_split].split(\" \") # Subject Split \n",
    "                    if any(s in arg_split for s in message_split): # Searching for alias names\n",
    "                        if self.u.get_id(r[arg_to_split]) == message_id:\n",
    "                            pred_list.append(r[other_arg])\n",
    "                            \n",
    "            self.q1.put(set(pred_list))\n",
    "        except RosetteException as exception:\n",
    "            print(exception)\n",
    "            self.q1.put(set(pred_list))\n",
    "\n",
    "\n",
    "    def ground_truth(self):\n",
    "#         u = Utils()\n",
    "        \n",
    "        pgt = set(self.u.ground_truth(self.relation, self.message))\n",
    "        self.q2.put(pgt)\n",
    "    \n",
    "    \n",
    "    def matrix_block(self):\n",
    "        if self.eid in self.rel_dict:\n",
    "            self.pgt = self.rel_dict[self.eid]['PGT']\n",
    "            self.extracted = self.rel_dict[self.eid]['Extracted']\n",
    "            self.contained = self.rel_dict[self.eid]['Contained']\n",
    "        else:\n",
    "            q1 = self.q1.get() # Extracted from API\n",
    "            q2 = self.q2.get() # PGT\n",
    "            #print(self.message, q1)\n",
    "            #print(self.message, q2)\n",
    "            self.pgt = len(q2)\n",
    "            self.extracted = len(q1)\n",
    "            q1 = [self.u.get_id(i) for i in q1]\n",
    "            q2 = [self.u.get_id(i) for i in q2]\n",
    "            #print(self.message, q1)\n",
    "            #print(self.message, q2)\n",
    "            count = 0\n",
    "            for i in q1:\n",
    "                if i in q2:\n",
    "                    count += 1\n",
    "            self.contained = count\n",
    "\n",
    "    def get_values(self):\n",
    "        if self.error:\n",
    "            raise Exception(self.error)\n",
    "        return [self.eid, self.message, self.extracted, self.contained, self.pgt]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Distribution(Thread):\n",
    "    def __init__(self, eid=None, name=None, lock=None, rel_dict={}):\n",
    "        Thread.__init__(self)\n",
    "        self.doc_len = None\n",
    "        self.u = Utils()\n",
    "        self.eid = eid\n",
    "        self.message = name\n",
    "        self.error = None\n",
    "        if name:\n",
    "            self.eid = self.u.get_id(name)\n",
    "        else:\n",
    "            self.message = self.u.id_to_name(eid)\n",
    "        if eid in rel_dict:\n",
    "            self.doc_len = rel_dict[eid]['Doc_Length']\n",
    "            return\n",
    "        self.start()\n",
    "    \n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                document = wikipedia.page(self.message).content\n",
    "                self.doc_len = len(document)\n",
    "                break\n",
    "            except wikipedia.DisambiguationError as e:\n",
    "                print(self.eid, self.message)\n",
    "                nameclash = True\n",
    "                for n in e.options:\n",
    "                    if self.u.get_id(n) == self.eid:\n",
    "                        if n == self.message:\n",
    "                            pass\n",
    "                        else:\n",
    "                            self.message = n\n",
    "                            nameclash = False\n",
    "                            break\n",
    "                if nameclash:\n",
    "                    self.message = \" \"\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                self.error = self.u.id_to_name(self.eid) + \" \" + str(e)\n",
    "                print (self.error)\n",
    "                break\n",
    "    \n",
    "    def get_values(self):\n",
    "        if self.error:\n",
    "            raise Exception(self.error)\n",
    "        return [self.eid, self.message, self.doc_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:18:05.448392Z",
     "start_time": "2019-03-07T15:18:05.443434Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_rel_dict(relation):\n",
    "    path = 'data/dumps/' + str(relation) + '.pkl'\n",
    "    relation_dict = {}\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            relation_dict = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"Creating a new Dictionary\")\n",
    "        relation_dict = {}\n",
    "    return relation_dict\n",
    "        \n",
    "    \n",
    "def save_rel_dict(relation, rel_dict):\n",
    "    path = 'data/dumps/' + str(relation) + '.pkl'\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(rel_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Done!\")\n",
    "        \n",
    "\n",
    "def rel_dict_to_df(relation):\n",
    "    rel_dict = load_rel_dict(relation)\n",
    "    df = pd.DataFrame.from_dict(rel_dict, orient='index')\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:18:08.603169Z",
     "start_time": "2019-03-07T15:18:08.572115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "with open(b'data/has_member_entity.ser') as fp:\n",
    "    data = fp.read()\n",
    "regex = '([Q][0-9]+)' # Regular Expression for the Entity IDs\n",
    "ids = re.findall(regex, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:18:09.089300Z",
     "start_time": "2019-03-07T15:18:09.080293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Wiki Pages by popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T13:31:31.244896Z",
     "start_time": "2019-03-07T13:24:41.119516Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "relation = 'Doc_Length' #Child\n",
    "topN_entities = ids[:n] #list(df['EntityID'][:n])\n",
    "u = Utils()\n",
    "\n",
    "### Creating threads for each Entity\n",
    "threads = []\n",
    "rel_dict = load_rel_dict(relation)\n",
    "for i in topN_entities:\n",
    "    threads.append( Distribution(eid=i, rel_dict=rel_dict) )\n",
    "\n",
    "### Waiting for each thread to complete\n",
    "for t in threads:\n",
    "    try:\n",
    "        t.join()\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "### Filling up the HeatMap\n",
    "table = PrettyTable(['EID', 'Name', 'Doc_Length'])\n",
    "for i,t in enumerate(threads):\n",
    "    try:\n",
    "        eid, name,doc_len = t.get_values()\n",
    "        rel_dict[eid] = {'Name':name, 'Doc_Length':doc_len}\n",
    "        table.add_row([eid, name, doc_len])\n",
    "    except:\n",
    "        print(\"Not saving in Dict\", i, ids[i], u.id_to_name(ids[i]))\n",
    "\n",
    "save_rel_dict(relation, rel_dict)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:18:21.833329Z",
     "start_time": "2019-03-07T15:18:21.626979Z"
    }
   },
   "outputs": [],
   "source": [
    "## Finding the probability of each ----> dummy: page length\n",
    "doc_len = rel_dict_to_df('Doc_Length')\n",
    "\n",
    "sum_len = np.sum(doc_len['Doc_Length'])\n",
    "doc_len['Prob'] = doc_len['Doc_Length']/sum_len\n",
    "\n",
    "dict_prob = doc_len['Prob'].to_dict() # Probability Dictionary\n",
    "dict_length = doc_len['Doc_Length'].to_dict() # Doc_length dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:18:34.560074Z",
     "start_time": "2019-03-07T15:18:34.557113Z"
    }
   },
   "outputs": [],
   "source": [
    "# n = 900\n",
    "# docs_to_pass = np.random.choice(list(dict_prob.keys()), n, p=list(dict_prob.values()), replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:18:55.062684Z",
     "start_time": "2019-03-07T15:18:35.895550Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~TheInfamousWayne/19.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='TheInfamousWayne', api_key='5o9cWieXjnukrtxb6sL3')\n",
    "# plotly.offline.init_notebook_mode(connected=True)\n",
    "import plotly.plotly as py\n",
    "# import plotly.plotly \n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "data_array = list(dict_length.values())\n",
    "hist_data = np.histogram(data_array, bins=24)\n",
    "binsize = hist_data[1][1] - hist_data[1][0]\n",
    "\n",
    "trace1 = go.Histogram(\n",
    "                        x=data_array,\n",
    "                        histnorm='',\n",
    "                        name='Histogram of Doc Length',\n",
    "                        autobinx=False,\n",
    "                        xbins=dict(\n",
    "                                    start=hist_data[1][0],\n",
    "                                    end=hist_data[1][-1],\n",
    "                                    size=binsize,\n",
    "                                    )\n",
    "                        )\n",
    "\n",
    "trace_data = [trace1]\n",
    "layout = go.Layout(\n",
    "                    bargroupgap=0.01\n",
    "                    )\n",
    "\n",
    "fig = go.Figure(data=trace_data, layout=layout)\n",
    "py.iplot(fig)#, filename='./doc_length.html', auto_open=True, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:19:15.831878Z",
     "start_time": "2019-03-07T15:19:15.826893Z"
    }
   },
   "outputs": [],
   "source": [
    "## Assigning bins to indivisual rows\n",
    "doc_list = list(dict_length.values())\n",
    "bins = hist_data[1]\n",
    "doc_len['BinGroup'] = list(np.digitize(doc_list,bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:19:17.495032Z",
     "start_time": "2019-03-07T15:19:17.483100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479\n",
      "70\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "## Generating multiple samples of different avg. page length\n",
    "setA = doc_len[doc_len['BinGroup'].isin([2,3,4])]\n",
    "setB = doc_len[doc_len['BinGroup'].isin([8,9,10])]\n",
    "setC = doc_len[doc_len['BinGroup'].isin([13,14,15,16,17,18,19,20,21,22,23,24])]\n",
    "\n",
    "sampleA = setA.sample(200)\n",
    "sampleB = setB.sample(70)\n",
    "sampleC = setC.sample(33)\n",
    "\n",
    "print(len(setA))\n",
    "print(len(setB))\n",
    "print(len(setC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Other Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T13:35:53.465930Z",
     "start_time": "2019-03-07T13:35:53.129390Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(list(dict_length.values()), hist=True, kde=False, \n",
    "             bins=12, color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})\n",
    "# Add labels\n",
    "plt.title('Document-Popularity Frequency-Distribution')\n",
    "plt.xlabel('Popularity')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T13:36:01.064264Z",
     "start_time": "2019-03-07T13:36:00.801306Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.kde import gaussian_kde\n",
    "from numpy import linspace\n",
    "\n",
    "data = list(dict_length.values())\n",
    "\n",
    "kde = gaussian_kde( data )\n",
    "dist_space = linspace( min(data), max(data), 100 )\n",
    "plt.plot( dist_space, kde(dist_space) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T13:36:05.493683Z",
     "start_time": "2019-03-07T13:36:05.251753Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "N = len(dict_length)\n",
    "n = N//10\n",
    "s = list(dict_length.values())   # generate your data sample with N elements\n",
    "p, x = np.histogram(s, bins=n) # bin it into n = N//10 bins\n",
    "x = x[:-1] + (x[1] - x[0])/2   # convert bin edges to centers\n",
    "f = UnivariateSpline(x, p, s=n)\n",
    "plt.plot(x, f(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeatMap Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T15:25:24.298812Z",
     "start_time": "2019-03-07T15:19:22.831380Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning:\n",
      "\n",
      "No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q25507 Accept\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning:\n",
      "\n",
      "No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1045752 Phoenix\n",
      "Accept Page id \" \" does not match any pages. Try another id!\n",
      "badRequestFormat: no content provided; must be one of an attachment, an inline \"content\" field, or an external \"contentUri\":\n",
      "  https://api.rosette.com/rest/v1/relationships\n",
      "Not saving in Dict 14 Q778278 Murderdolls\n",
      "Done!\n",
      "+--------------------------------------+-----------+-----+-----------+\n",
      "|                 Name                 | Extracted | PGT | Contained |\n",
      "+--------------------------------------+-----------+-----+-----------+\n",
      "|              Crazy Town              |     1     |  1  |     0     |\n",
      "|               Hard-Fi                |     0     |  1  |     0     |\n",
      "|              April Wine              |     2     |  1  |     0     |\n",
      "|                3OH!3                 |     0     |  1  |     0     |\n",
      "|             All Time Low             |     1     |  1  |     0     |\n",
      "|                Sistar                |     0     |  4  |     0     |\n",
      "|             Procol Harum             |     3     |  1  |     0     |\n",
      "|           Theodor Bastard            |     0     |  2  |     0     |\n",
      "|              The Kills               |     1     |  1  |     1     |\n",
      "|            The Jackson 5             |     2     |  5  |     0     |\n",
      "|          The Style Council           |     0     |  4  |     0     |\n",
      "|               A-Teens                |     0     |  0  |     0     |\n",
      "|             Combichrist              |     2     |  1  |     0     |\n",
      "|              The Wanted              |     0     |  1  |     0     |\n",
      "|              Icona Pop               |     0     |  2  |     0     |\n",
      "|         The Pretty Reckless          |     0     |  0  |     0     |\n",
      "|              Eluveitie               |     0     |  1  |     0     |\n",
      "|              Goldfrapp               |     2     |  2  |     1     |\n",
      "|    Quicksilver Messenger Service     |     0     |  8  |     0     |\n",
      "|         La Oreja de Van Gogh         |     0     |  6  |     0     |\n",
      "|               Outlawz                |     12    |  9  |     5     |\n",
      "|           The Dead Weather           |     1     |  1  |     1     |\n",
      "|           The Raveonettes            |     0     |  2  |     0     |\n",
      "|               Scandal                |     0     |  4  |     0     |\n",
      "|            Cheech & Chong            |     0     |  2  |     0     |\n",
      "|            The Dubliners             |     0     |  0  |     0     |\n",
      "|                Helmet                |     0     |  2  |     0     |\n",
      "|                ZZ Top                |     2     |  3  |     0     |\n",
      "|          Ocean Colour Scene          |     1     |  1  |     0     |\n",
      "|              Airbourne               |     1     |  1  |     0     |\n",
      "|               Boyzone                |     2     |  1  |     0     |\n",
      "|            Amberian Dawn             |     2     |  0  |     0     |\n",
      "|             Bright Eyes              |     3     |  3  |     1     |\n",
      "|              The Donnas              |     0     |  4  |     0     |\n",
      "|            Bad Meets Evil            |     0     |  2  |     0     |\n",
      "|          The Dandy Warhols           |     0     |  1  |     0     |\n",
      "|       Five Finger Death Punch        |     5     |  1  |     0     |\n",
      "|         Theory of a Deadman          |     0     |  1  |     0     |\n",
      "|               78violet               |     0     |  2  |     0     |\n",
      "|              Sugarland               |     0     |  1  |     0     |\n",
      "|            Phantom Planet            |     0     |  1  |     0     |\n",
      "|              Sentenced               |     0     |  6  |     0     |\n",
      "|             The Gap Band             |     0     |  1  |     0     |\n",
      "|             New Edition              |     4     |  6  |     2     |\n",
      "|         5 Seconds of Summer          |     0     |  4  |     0     |\n",
      "|             Dimmu Borgir             |     2     |  1  |     1     |\n",
      "|             Morbid Angel             |     0     |  1  |     0     |\n",
      "|             The Bravery              |     1     |  1  |     0     |\n",
      "|              Arch Enemy              |     5     |  1  |     0     |\n",
      "|             Deep Forest              |     3     |  1  |     0     |\n",
      "|             Babyshambles             |     5     |  1  |     0     |\n",
      "|             3 Doors Down             |     0     |  1  |     0     |\n",
      "|                Saliva                |     0     |  2  |     0     |\n",
      "|            Virgin Steele             |     0     |  2  |     0     |\n",
      "|              Buzzcocks               |     1     |  3  |     0     |\n",
      "|               Seether                |     2     |  1  |     1     |\n",
      "|            Fates Warning             |     0     |  1  |     0     |\n",
      "|            Monster Magnet            |     1     |  1  |     0     |\n",
      "|            Modern Talking            |     2     |  2  |     2     |\n",
      "|               Local H                |     2     |  1  |     0     |\n",
      "|       Corrosion of Conformity        |     4     |  1  |     0     |\n",
      "|              Hüsker Dü               |     0     |  3  |     0     |\n",
      "|              Finntroll               |     0     |  1  |     0     |\n",
      "|       Clap Your Hands Say Yeah       |     0     |  1  |     0     |\n",
      "|               Thursday               |     0     |  1  |     0     |\n",
      "|             Dinosaur Jr.             |     0     |  1  |     0     |\n",
      "|               En Vogue               |     0     |  1  |     0     |\n",
      "|             Circle Jerks             |     0     |  1  |     0     |\n",
      "|             The Ventures             |     5     |  1  |     0     |\n",
      "|              The Coral               |     0     |  1  |     0     |\n",
      "|           Type O Negative            |     0     |  1  |     0     |\n",
      "|                 Týr                  |     0     |  1  |     0     |\n",
      "|               The Fray               |     0     |  4  |     0     |\n",
      "|                  R5                  |     1     |  5  |     1     |\n",
      "|              The Shins               |     2     |  1  |     1     |\n",
      "|             Shadows Fall             |     5     |  1  |     1     |\n",
      "|          Poets of the Fall           |     0     |  1  |     0     |\n",
      "|           The Hellacopters           |     1     |  1  |     0     |\n",
      "|             DragonForce              |     2     |  1  |     0     |\n",
      "|             Freedom Call             |     1     |  1  |     0     |\n",
      "|              HammerFall              |     5     |  1  |     0     |\n",
      "|         Asian Dub Foundation         |     0     |  1  |     0     |\n",
      "|        Them Crooked Vultures         |     0     |  1  |     0     |\n",
      "|               Gugudan                |     0     |  9  |     0     |\n",
      "|         Cavalera Conspiracy          |     4     |  6  |     2     |\n",
      "|              Metronomy               |     0     |  4  |     0     |\n",
      "|            Reel Big Fish             |     0     |  1  |     0     |\n",
      "|             Leaves' Eyes             |     0     |  1  |     0     |\n",
      "|               Loudness               |     0     |  5  |     0     |\n",
      "|           Die Toten Hosen            |     0     |  5  |     0     |\n",
      "|      Frankie Goes to Hollywood       |     0     |  1  |     0     |\n",
      "|                LMFAO                 |     0     |  2  |     0     |\n",
      "|                 Aria                 |     0     |  2  |     0     |\n",
      "|                Kittie                |     4     |  2  |     0     |\n",
      "|                 Fun                  |     0     |  2  |     0     |\n",
      "|              Stray Cats              |     0     |  1  |     0     |\n",
      "|              Mazzy Star              |     1     |  2  |     0     |\n",
      "|               Flyleaf                |     0     |  2  |     0     |\n",
      "|          The Postal Service          |     1     |  3  |     1     |\n",
      "|                Cream                 |     0     |  3  |     0     |\n",
      "|               Kon Kan                |     0     |  1  |     0     |\n",
      "|             Lacuna Coil              |     1     |  1  |     0     |\n",
      "|          The Avett Brothers          |     2     |  3  |     0     |\n",
      "|            Do As Infinity            |     1     |  1  |     0     |\n",
      "|           Green Carnation            |     0     |  1  |     0     |\n",
      "|           Rhapsody of Fire           |     5     |  10 |     5     |\n",
      "|            Stereophonics             |     0     |  1  |     0     |\n",
      "|          C+C Music Factory           |     0     |  1  |     0     |\n",
      "|            Skunk Anansie             |     0     |  1  |     0     |\n",
      "|         Angus & Julia Stone          |     1     |  2  |     0     |\n",
      "|        Rocket from the Crypt         |     0     |  1  |     0     |\n",
      "|               Klaxons                |     0     |  2  |     0     |\n",
      "|            You Me at Six             |     0     |  1  |     0     |\n",
      "|               G-Friend               |     0     |  6  |     0     |\n",
      "|            The Fratellis             |     1     |  1  |     1     |\n",
      "|          Naughty by Nature           |     1     |  1  |     0     |\n",
      "|                Yazoo                 |     0     |  1  |     0     |\n",
      "|               Mudvayne               |     6     |  1  |     1     |\n",
      "|          Silversun Pickups           |     0     |  1  |     0     |\n",
      "|              Noir Désir              |     0     |  4  |     0     |\n",
      "|            Cocteau Twins             |     2     |  4  |     1     |\n",
      "|            The Raconteurs            |     0     |  1  |     0     |\n",
      "|   Tom Petty and the Heartbreakers    |     0     |  8  |     0     |\n",
      "|                P.O.D.                |     0     |  1  |     0     |\n",
      "|     Nick Cave and the Bad Seeds      |     0     |  7  |     0     |\n",
      "|             Major Lazer              |     0     |  3  |     0     |\n",
      "|               Nemesea                |     2     |  1  |     0     |\n",
      "|            In This Moment            |     2     |  0  |     0     |\n",
      "|            Phoenix (band)            |     0     |  1  |     0     |\n",
      "|         Death Cab for Cutie          |     0     |  6  |     0     |\n",
      "|           Vampire Weekend            |     0     |  4  |     0     |\n",
      "|             The Bar-Kays             |     0     |  1  |     0     |\n",
      "|                NSYNC                 |     2     |  5  |     1     |\n",
      "|               Brujeria               |     0     |  1  |     0     |\n",
      "|              Halestorm               |     0     |  0  |     0     |\n",
      "|           Gym Class Heroes           |     0     |  2  |     0     |\n",
      "|          Angels & Airwaves           |     0     |  1  |     0     |\n",
      "|             Steppenwolf              |     0     |  6  |     0     |\n",
      "|           Pierce the Veil            |     3     |  1  |     1     |\n",
      "|           Ozric Tentacles            |     0     |  1  |     0     |\n",
      "|              The Rasmus              |     0     |  4  |     0     |\n",
      "|         Black Label Society          |     1     |  1  |     0     |\n",
      "|                Sweet                 |     0     |  4  |     0     |\n",
      "|          The Durutti Column          |     0     |  1  |     0     |\n",
      "|              Morcheeba               |     0     |  2  |     0     |\n",
      "|         The Magnetic Fields          |     0     |  4  |     0     |\n",
      "|           Wilson Phillips            |     0     |  1  |     0     |\n",
      "|              The B-52's              |     1     |  1  |     0     |\n",
      "|            Casting Crowns            |     0     |  1  |     0     |\n",
      "|         Peter Bjorn and John         |     0     |  1  |     0     |\n",
      "|              Biohazard               |     0     |  1  |     0     |\n",
      "|             Passion Pit              |     1     |  1  |     1     |\n",
      "|         The Dave Clark Five          |     0     |  5  |     0     |\n",
      "|            New York Dolls            |     0     |  1  |     0     |\n",
      "|             Supersuckers             |     0     |  1  |     0     |\n",
      "| Edward Sharpe and the Magnetic Zeros |     0     |  1  |     0     |\n",
      "|                Burzum                |     0     |  1  |     0     |\n",
      "|             The Runaways             |     2     |  2  |     1     |\n",
      "|              Wolfmother              |     2     |  1  |     1     |\n",
      "|                 Cute                 |     0     |  8  |     0     |\n",
      "|            Cold War Kids             |     2     |  1  |     1     |\n",
      "|             Alexisonfire             |     1     |  1  |     0     |\n",
      "|            The Lumineers             |     0     |  3  |     0     |\n",
      "|                 EXID                 |     2     |  5  |     1     |\n",
      "|             Die Antwoord             |     1     |  2  |     0     |\n",
      "|             Annihilator              |     0     |  1  |     0     |\n",
      "|               Soilwork               |     1     |  7  |     1     |\n",
      "|           Mott the Hoople            |     0     |  1  |     0     |\n",
      "|              Supergrass              |     1     |  1  |     0     |\n",
      "|                 Muse                 |     0     |  3  |     0     |\n",
      "|           Boys Like Girls            |     0     |  1  |     0     |\n",
      "|          Kaizers Orchestra           |     0     |  1  |     0     |\n",
      "|                 Saga                 |     0     |  4  |     0     |\n",
      "|             Wishbone Ash             |     3     |  5  |     2     |\n",
      "|                Keane                 |     0     |  1  |     0     |\n",
      "|                Krokus                |     2     |  1  |     1     |\n",
      "|          3 Inches of Blood           |     0     |  6  |     0     |\n",
      "|              Alphaville              |     0     |  5  |     0     |\n",
      "|              Noisettes               |     1     |  1  |     0     |\n",
      "|              Apollo 440              |     0     |  1  |     0     |\n",
      "|                Ska-P                 |     1     |  4  |     0     |\n",
      "|      Between the Buried and Me       |     0     |  1  |     0     |\n",
      "|              The Pogues              |     3     |  1  |     1     |\n",
      "|           Jimmy Eat World            |     0     |  1  |     0     |\n",
      "|        Eagles of Death Metal         |     0     |  2  |     0     |\n",
      "|           Collective Soul            |     4     |  1  |     0     |\n",
      "|              Los Lobos               |     0     |  3  |     0     |\n",
      "|              Eiffel 65               |     0     |  3  |     0     |\n",
      "|       Gladys Knight & the Pips       |     0     |  1  |     0     |\n",
      "|            Vanilla Fudge             |     2     |  1  |     0     |\n",
      "|            Sonata Arctica            |     0     |  9  |     0     |\n",
      "|          Temple of the Dog           |     1     |  6  |     1     |\n",
      "|                Camila                |     0     |  1  |     0     |\n",
      "|        Death from Above 1979         |     1     |  1  |     0     |\n",
      "|         The Tennessee Three          |     2     |  1  |     0     |\n",
      "|            Iron Butterfly            |     3     |  5  |     0     |\n",
      "|         Modern Jazz Quartet          |     0     |  1  |     0     |\n",
      "|           Babes in Toyland           |     3     |  2  |     1     |\n",
      "|               Angerme                |     4     |  15 |     3     |\n",
      "+--------------------------------------+-----------+-----+-----------+\n"
     ]
    }
   ],
   "source": [
    "matrix_extr = np.zeros([20,20]) ## The HeatMap for all extracted Relations by Rosette\n",
    "matrix_cont = np.zeros([20,20]) ## The HeatMap for all contained (in PGT) relations\n",
    "lock = Lock() ## Shared Lock\n",
    "sample_to_pass = sampleA\n",
    "n = len(sample_to_pass)\n",
    "relation = 'Person Employee or Member of^-1' #Band Members\n",
    "# topN_entities = ids[:n]#list(df['EntityID'][:n])\n",
    "topN_entities = list(sample_to_pass.index)\n",
    "u = Utils()\n",
    "\n",
    "### Creating threads for each Entity\n",
    "threads = []\n",
    "rel_dict = load_rel_dict(relation)\n",
    "for i in topN_entities:\n",
    "    threads.append( HeatMaps(lock, eid=i, relation=relation, rel_dict=rel_dict) )\n",
    "\n",
    "### Waiting for each thread to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "\n",
    "### Filling up the HeatMap\n",
    "table = PrettyTable(['Name', 'Extracted', 'PGT', 'Contained'])\n",
    "for i,t in enumerate(threads):\n",
    "    try:\n",
    "        eid, name,r_ext,r_cont,c = t.get_values()\n",
    "        rel_dict[eid] = {'EntityName':name, 'Extracted':r_ext, 'PGT':c, 'Contained':r_cont}\n",
    "        table.add_row([name,r_ext,c, r_cont])\n",
    "        matrix_cont[ r_cont, c] += 1\n",
    "        matrix_extr[ r_ext, c] += 1\n",
    "    except:\n",
    "        print(\"Not saving in Dict\", i, ids[i], u.id_to_name(ids[i]))\n",
    "\n",
    "save_rel_dict(relation, rel_dict)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:33.311046Z",
     "start_time": "2019-03-05T18:21:33.308041Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 20, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:44.194440Z",
     "start_time": "2019-03-05T18:21:39.751885Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "# Contained\n",
    "sns.heatmap(matrix_cont, ax=ax1, fmt='.0f', cmap='gist_gray_r', xticklabels = [\"\"], yticklabels = [\"\"], annot = True, cbar_kws={\"orientation\": \"horizontal\"})\n",
    "ax1.set_ylabel('Rosette')    \n",
    "ax1.set_xlabel('PGT')\n",
    "ax1.set_title('Contained(in PGT) Vs. PGT')\n",
    "\n",
    "# Extracted\n",
    "sns.heatmap(matrix_extr, ax=ax2, fmt='.0f', cmap='gist_gray_r', xticklabels = [\"\"], yticklabels = [\"\"], annot = True, cbar_kws={\"orientation\": \"horizontal\"})\n",
    "ax2.set_ylabel('Rosette')    \n",
    "ax2.set_xlabel('PGT')\n",
    "ax2.set_title('Extracted(API) Vs. PGT')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"data/img/{}-{}.png\".format(relation,n))\n",
    "\n",
    "matrix_cont.dump('data/numpy_matrices/{}-{}-matrix_cont.dat'.format(relation,n))\n",
    "matrix_extr.dump('data/numpy_matrices/{}-{}-matrix_extr.dat'.format(relation,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T18:21:45.139191Z",
     "start_time": "2019-03-05T18:21:44.198460Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig1 = sns.heatmap(matrix_cont, linewidth=0.5)\n",
    "fig2 = sns.heatmap(matrix_extr, linewidth=0.5)\n",
    "\n",
    "fig1.set_title('Contained(in PGT) Vs. PGT')\n",
    "fig2.set_title('Extracted(API) Vs. PGT')\n",
    "fig1.set(xlabel='PGT', ylabel='API')\n",
    "fig2.set(xlabel='PGT', ylabel='API')\n",
    "\n",
    "# fig1.get_figure().savefig(\"data/img/{}-{}-matrix_cont.png\".format(relation,n))\n",
    "# fig2.get_figure().savefig(\"data/img/{}-{}-matrix_extr.png\".format(relation,n))\n",
    "\n",
    "matrix_cont.dump('data/numpy_matrices/{}-{}-matrix_cont.dat'.format(relation,n))\n",
    "matrix_extr.dump('data/numpy_matrices/{}-{}-matrix_extr.dat'.format(relation,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T11:36:12.580006Z",
     "start_time": "2019-03-01T11:36:11.989100Z"
    }
   },
   "outputs": [],
   "source": [
    "fig1.set(xlabel='PGT', ylabel='API')\n",
    "fig1.set_title('Contained(in PGT) Vs. PGT')\n",
    "fig1.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T11:36:13.164241Z",
     "start_time": "2019-03-01T11:36:12.583265Z"
    }
   },
   "outputs": [],
   "source": [
    "fig2.set(xlabel='PGT', ylabel='API')\n",
    "fig2.set_title('Extracted(API) Vs. PGT')\n",
    "fig2.get_figure()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Child, Educated, Spouse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
