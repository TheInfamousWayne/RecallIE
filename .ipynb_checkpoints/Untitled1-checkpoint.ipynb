{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:45:19.467778Z",
     "start_time": "2019-04-01T00:45:19.463821Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from random import randint\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:45:20.290928Z",
     "start_time": "2019-04-01T00:45:20.218155Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load news/utils.py\n",
    "QUERY_DICT = {'Organization Founded By^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          ?item wdt:P112 wd:%s.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                           ],\n",
    "              'Organization Founded By':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P112 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                        ],\n",
    "              'Organization Headquarters':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P159 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                          ],\n",
    "              'Organization Subsidiary Of^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P355 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                              ],\n",
    "              'Organization Subsidiary Of':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          ?item wdt:P355 wd:%s.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\"\n",
    "                                           ],\n",
    "              'Organization top employees':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P169 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\", # CEO\n",
    "                                            \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P488 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\" # Chairperson\n",
    "                                            ],\n",
    "              'Person Employee or Member of^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          ?item wdt:P108 wd:%s.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\",\n",
    "                                            \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                          wd:%s wdt:P527 ?item.\n",
    "                                          SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                        }\"\"\" \n",
    "                                                ],\n",
    "              'Person Employee or Member of':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P108 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\",\n",
    "                                              \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P463 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"## member of ---> Band Members\n",
    "                                            ],\n",
    "              'Person Place of Birth':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P19 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                      ],\n",
    "              'Person Current and Past Location of Residence':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P551 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                                              ],\n",
    "              'Person Parents':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P22 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\", #Father\n",
    "                                \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P25 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\", #Mother\n",
    "                                \"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P1038 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\" #Relative (Adopted Parents?)\n",
    "                                # Shall we include stepparents??\n",
    "                               ],\n",
    "              'Person Parents^-1':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P40 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                  ],\n",
    "              'Person Siblings':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P3373 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                                ],\n",
    "              'Person Spouse':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P26 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                              ],\n",
    "              'Citizen of':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P27 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                           ],\n",
    "              'Educated at':[\"\"\"SELECT ?item ?itemLabel WHERE {\n",
    "                                              wd:%s wdt:P69 ?item.\n",
    "                                              SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                                            }\"\"\"\n",
    "                            ]\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "PERSON_RELATIONS = ['Educated at', 'Citizen of', 'Person Employee or Member of', 'Organization top employees^-1',\\\n",
    "                    'Person Current and Past Location of Residence', 'Person Parents', 'Person Parents^-1',\\\n",
    "                    'Person Place of Birth', 'Person Siblings', 'Person Spouse']\n",
    "ORG_RELATION =     ['Organization Founded By', 'Organization Collaboration', 'Organization Collaboration^-1',\\\n",
    "                    'Organization Headquarters', 'Organization Subsidiary Of', 'Organization Subsidiary Of^-1',\\\n",
    "                    'Organization top employees', 'Person Employee or Member of^-1', 'Organization Acquired By^-1',\\\n",
    "                    'Organization Acquired By', 'Organization Provider To', 'Organization Provider To^-1']\n",
    "COMMON_RELATION =  ['Organization Founded By^-1']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON   \n",
    "from rosette.api import API, DocumentParameters, RosetteException\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "import requests\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from threading import Lock\n",
    "import os, sys\n",
    "import threading\n",
    "from threading import Thread\n",
    "import queue\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "from lxml import html\n",
    "from datetime import date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "def count_confidence(main_df):\n",
    "\t    if (not main_df.empty):\n",
    "\t        main_df = main_df.sort_values('Object', ascending=True).drop_duplicates().groupby(['Subject','Relationship']).agg(lambda x: list(x))\n",
    "\t        main_df['Object'] = main_df['Object'].agg(lambda x: x if x != [''] else [])\n",
    "\t        main_df['Count'] = main_df['Object'].apply(lambda x: len(x))\n",
    "\t        #main_df = main_df[[c for c in main_df if c not in ['Confidence']] + ['Confidence']]\n",
    "\t    return main_df\n",
    "\n",
    "def load_useful100(query):\n",
    "    path = 'data/dumps/{}_useful100.pkl'.format(query)\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            useful100 = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"No file exists. Creating a new one.\")\n",
    "        useful100 = {}\n",
    "    return useful100\n",
    "\n",
    "\n",
    "def load_headline_dict(query):\n",
    "    path = 'data/dumps/{}_headline_dict.pkl'.format(query)\n",
    "    headline_dict = {}\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            headline_dict = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"Creating a new Dictionary\")\n",
    "        headline_dict = {}\n",
    "    return headline_dict\n",
    "\n",
    "def get_news(link):\n",
    "    ### Setting up API ###\n",
    "    options = Options()\n",
    "    options.binary_location = \"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--window-size=1200x600')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "\n",
    "    chromedriver = 'C:\\\\Users\\\\Bhavya\\\\Desktop\\\\Vaibhav\\\\chromedriver.exe'\n",
    "    r = webdriver.Chrome(executable_path=chromedriver, options=options)\n",
    "\n",
    "    ### Scraping From URL ###\n",
    "    r.get(link)\n",
    "    doc_summary = []\n",
    "    soup = BeautifulSoup(r.page_source, 'lxml')\n",
    "    all_paragraphs = soup.findAll(\"p\")\n",
    "    for paragraph in all_paragraphs:\n",
    "        doc_summary.append(paragraph.text)\n",
    "    doc_summary = \" \".join(doc_summary)\n",
    "    doc_summary = \" \".join(doc_summary.split())\n",
    "    return doc_summary\n",
    "\n",
    "class Utils:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.id_dict = {}\n",
    "        self.lock = Lock()\n",
    "        self.load_dict()\n",
    "        \n",
    "    \n",
    "    def __del__(self):\n",
    "        self.save_dict()\n",
    "\n",
    "    def get_id(self, message, dict_to_use=None):\n",
    "#         if dict_to_use:\n",
    "#             dict_to_use = dict_to_use\n",
    "#         else:\n",
    "#             global id_dict\n",
    "#             dict_to_use = id_dict\n",
    "    \n",
    "        if message in self.id_dict:\n",
    "            return self.id_dict[message]\n",
    "        else:\n",
    "            API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "            query = message\n",
    "            params = {\n",
    "                'action': 'wbsearchentities',\n",
    "                'format': 'json',\n",
    "                'language': 'en',\n",
    "                'search': query\n",
    "            }\n",
    "            r = requests.get(API_ENDPOINT, params = params)\n",
    "            try:\n",
    "                with self.lock:\n",
    "                    self.id_dict[message] = r.json()['search'][0]['id']\n",
    "                return self.id_dict[message]\n",
    "            except Exception:\n",
    "                return -1 #The id doesn't exist\n",
    "\n",
    "\n",
    "    def id_to_name(self, eid):\n",
    "#         if dict_to_use:\n",
    "#             dict_to_use = dict_to_use\n",
    "#         else:\n",
    "#             global id_dict\n",
    "#             dict_to_use = id_dict\n",
    "\n",
    "        if eid in self.id_dict.values():\n",
    "            return [key for key, value in self.id_dict.items() if value == eid][0]\n",
    "        else:\n",
    "            API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "            query = eid\n",
    "            params = {\n",
    "                'action': 'wbsearchentities',\n",
    "                'format': 'json',\n",
    "                'language': 'en',\n",
    "                'search': query\n",
    "            }\n",
    "            r = requests.get(API_ENDPOINT, params = params)\n",
    "            try:\n",
    "                with self.lock:\n",
    "                    self.id_dict[ r.json()['search'][0]['label'] ] = r.json()['search'][0]['id']\n",
    "                return r.json()['search'][0]['label']\n",
    "            except Exception:\n",
    "                return -1 #The id doesn't exist\n",
    "\n",
    "\n",
    "    def get_results(self, query, value, endpoint_url=\"https://query.wikidata.org/sparql\"):\n",
    "        sparql = SPARQLWrapper(endpoint_url)\n",
    "        sparql.setQuery(query%value)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        return sparql.query().convert()\n",
    "\n",
    "\n",
    "    def ground_truth(self, relation, subject, debug=False):\n",
    "        global QUERY_DICT\n",
    "        results = []\n",
    "        gt = []\n",
    "        try:\n",
    "            results = [self.get_results(query, self.get_id(subject)) for query in QUERY_DICT[relation]]\n",
    "            for result in results:\n",
    "                for r in result[\"results\"][\"bindings\"]:\n",
    "                    gt.append(r['itemLabel']['value'])\n",
    "        except:\n",
    "            if debug:\n",
    "                print (relation, subject)\n",
    "        return gt\n",
    "\n",
    "    def add_ground_truth(self, df, debug=False):\n",
    "        if df.empty:\n",
    "            return df\n",
    "        if debug:\n",
    "            print (df)\n",
    "        df = df.reset_index()\n",
    "        df['Pseudo Ground Truth'] = df.apply(lambda row: self.ground_truth(row['Relationship'], row['Subject']), axis=1)\n",
    "        df['Count_PGT'] = df['Pseudo Ground Truth'].apply(lambda x: len(x))\n",
    "        df = df.set_index(['Subject','Relationship'])\n",
    "        return df\n",
    "\n",
    "    def add_recall_score(self, df):\n",
    "        df['Recall Prediction'] = np.random.randint(0, 100, df.shape[0])/100\n",
    "        return df\n",
    "\n",
    "\n",
    "    def load_dict(self):\n",
    "        try:\n",
    "            with open('data/dumps/id_dict.pkl', 'rb') as fp:\n",
    "                self.id_dict = pickle.load(fp)\n",
    "        except:\n",
    "            print (\"Creating a new Dictionary\")\n",
    "            self.id_dict = {}\n",
    "\n",
    "\n",
    "    def save_dict(self):\n",
    "        with self.lock:\n",
    "            old_dict = self.get_dict()\n",
    "            self.id_dict = {**self.id_dict, **old_dict}\n",
    "            with open('data/dumps/id_dict.pkl', 'wb') as fp:\n",
    "                pickle.dump(self.id_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"Saved\")\n",
    "\n",
    "\n",
    "    def get_dict(self):\n",
    "        di = {}\n",
    "        with open('data/dumps/id_dict.pkl', 'rb') as fp:\n",
    "            di = pickle.load(fp)\n",
    "        return di\n",
    "\n",
    "\n",
    "    def Analyse(self, message, doc=None, lock=None, alt_url='https://api.rosette.com/rest/v1/'):\n",
    "        \"\"\" Run the example \"\"\"\n",
    "        # Create an API instance\n",
    "        api = API(user_key=\"969b3593686184bb42803d8da453f119\", service_url=alt_url)\n",
    "\n",
    "        # Set selected API options.\n",
    "        # For more information on the functionality of these\n",
    "        # and other available options, see Rosette Features & Functions\n",
    "        # https://developer.rosette.com/features-and-functions#morphological-analysis-introduction\n",
    "\n",
    "        # api.set_option('modelType','perceptron') #Valid for Chinese and Japanese only\n",
    "\n",
    "        # Opening the ID Dictionary\n",
    "#         load_dict()\n",
    "        ### Will Close after Analysis of the document is completed\n",
    "    \n",
    "        if lock == None:\n",
    "            lock = Lock()\n",
    "\n",
    "        params = DocumentParameters()\n",
    "        if doc:\n",
    "            relationships_text_data = doc[:20000]\n",
    "        else:\n",
    "            relationships_text_data = wikipedia.page(message).content[:20000]\n",
    "        params[\"content\"] = relationships_text_data\n",
    "        rel = []\n",
    "        message_id = self.get_id(message)\n",
    "        message_split = message.split(\" \")\n",
    "        try:\n",
    "            with lock:\n",
    "                RESULT = api.relationships(params)\n",
    "            \n",
    "            for r in RESULT['relationships']:\n",
    "                arg2_split = r['arg2'].split(\" \")\n",
    "                confidence = '?'\n",
    "                if \"confidence\" in r:\n",
    "                    confidence = str(round(r[\"confidence\"],2))\n",
    "                if any(s in arg2_split for s in message_split):\n",
    "                    if self.get_id(r['arg2']) == message_id:\n",
    "                        rel.append({'Relationship':r['predicate']+'^-1', 'Subject':r['arg2'], 'Object':r['arg1'], 'Confidence': confidence})\n",
    "                rel.append({'Relationship':r['predicate'],'Subject':r['arg1'],'Object':r['arg2'], 'Confidence': confidence})\n",
    "\n",
    "            ## Closing the ID Dict\n",
    "            self.save_dict()\n",
    "            ##\n",
    "            return rel, message_id\n",
    "        except RosetteException as exception:\n",
    "            print(exception)\n",
    "\n",
    "\n",
    "\n",
    "class News(Thread):\n",
    "    def __init__(self, lock=None, link=None, name=None, shared_df=None, headline=None):\n",
    "        Thread.__init__(self)\n",
    "        self.df = pd.DataFrame()\n",
    "        self.main_df = pd.DataFrame()\n",
    "        self.u = Utils()\n",
    "        self.headline = headline\n",
    "        self.lock = lock\n",
    "        self.link = link\n",
    "        self.eid = self.u.get_id(name)\n",
    "        self.doc = \"\"\n",
    "        self.message = name\n",
    "        self.start()\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        a = Thread(target = self.get_link_text, args = ())\n",
    "        a.start()\n",
    "        a.join()\n",
    "\n",
    "        \n",
    "    def get_link_text(self):\n",
    "        r = requests.get(self.link)\n",
    "        content = r.text\n",
    "        doc_summary = []\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        paras = soup.findAll(\"p\")\n",
    "        for p in paras:\n",
    "            doc_summary.append(p.text)\n",
    "        s = \" \".join(doc_summary)\n",
    "        s = \" \".join(s.split())\n",
    "        self.doc = s\n",
    "        ## Analysing the link text\n",
    "        res,_ = self.u.Analyse(message=self.message, doc=self.doc, lock=self.lock)\n",
    "        self.df = pd.DataFrame(res, columns=['Subject','Relationship','Object','Confidence'])\n",
    "        self.main_df = self.df[self.df['Subject'].apply(lambda row: self.u.get_id(row)) == self.eid]\n",
    "        self.main_df['Subject'] = self.message\n",
    "    \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def get_main_df(self):\n",
    "        return self.headline, self.link, self.main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:45:21.878533Z",
     "start_time": "2019-04-01T00:45:21.873546Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "u = Utils()\n",
    "queries = ['Angela Merkel', 'Donald Trump', 'Ivanka', 'Bill Gates']#, 'Brad Pitt', 'Christopher Nolan', 'Christian Bale', 'Megan Fox', 'Steve Jobs', 'Hillary Clinton', 'Gerhard Weikum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:45:22.940479Z",
     "start_time": "2019-04-01T00:45:22.868704Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_headline_dict(query):\n",
    "    ### Getting Custom Date Range ###\n",
    "    y1,m1,d1 = ('2014','01','01')\n",
    "    y2,m2,d2 = str(date.today()).split('-')\n",
    "    \n",
    "    ### Setting up API ###\n",
    "    headline_list = []\n",
    "    headline_dict = {}\n",
    "\n",
    "    options = Options()\n",
    "    options.binary_location = \"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--window-size=1200x600')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "\n",
    "    chromedriver = 'C:\\\\Users\\\\Bhavya\\\\Desktop\\\\Vaibhav\\\\chromedriver.exe'\n",
    "    r = webdriver.Chrome(executable_path=chromedriver, options=options)\n",
    "\n",
    "    ### Scraping From Google ###\n",
    "    final_date = date(int(y2),int(m2),int(d2))\n",
    "    while True:\n",
    "        dd = int(d1)\n",
    "        mm = int(m1) \n",
    "        yy = int(y1) + mm//12\n",
    "        mm = mm % 12 + 1    \n",
    "\n",
    "        temp_date = date(yy,mm,dd)   \n",
    "        if temp_date > final_date:\n",
    "            break  \n",
    "\n",
    "        r.get(\"https://www.google.com/search?q={}&hl=en-US&gl=US&source=lnt&tbs=cdr%3A1%2Ccd_min%3A{}%2F{}%2F{}%2Ccd_max%3A{}%2F{}%2F{}&tbm=nws\".format(\\\n",
    "                            query,m1,d1,y1,mm,dd,yy))\n",
    "        soup = BeautifulSoup(r.page_source, 'lxml')\n",
    "        all_headlines = soup.findAll(\"h3\")\n",
    "        for headline in all_headlines:\n",
    "            headline_list.append(headline.text)\n",
    "            headline_dict[headline.text] = headline.a['href']        \n",
    "        d1,m1,y1 = dd,mm,yy\n",
    "    \n",
    "    ### Saving the file ###\n",
    "    save_headline_dict(headline_dict)\n",
    "\n",
    "def save_headline_dict(headline_dict):\n",
    "    path = 'data/dumps/{}_headline_dict.pkl'.format(query)\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(headline_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"headline_dict saved!\")\n",
    "        \n",
    "def load_headline_dict(query):\n",
    "    path = 'data/dumps/{}_headline_dict.pkl'.format(query)\n",
    "    headline_dict = {}\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            headline_dict = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"Creating a new Dictionary\")\n",
    "        headline_dict = {}\n",
    "    return headline_dict\n",
    "\n",
    "def save_useful100(useful100):\n",
    "    path = 'data/dumps/{}_useful100.pkl'.format(query)\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(useful100, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"100 useful headlines for query {} is saved!\".format(query))\n",
    "        \n",
    "def load_useful100(query):\n",
    "    path = 'data/dumps/{}_useful100.pkl'.format(query)\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            useful100 = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"No file exists. Creating a new one for {}\".format(query))\n",
    "        useful100 = {}\n",
    "    return useful100\n",
    "\n",
    "PERSON_RELATIONS = ['Educated at', 'Citizen of', 'Person Employee or Member of', 'Organization top employees^-1',\\\n",
    "                    'Person Current and Past Location of Residence', 'Person Parents', 'Person Parents^-1',\\\n",
    "                    'Person Place of Birth', 'Person Siblings', 'Person Spouse']\n",
    "ORG_RELATION =     ['Organization Founded By', 'Organization Collaboration', 'Organization Collaboration^-1',\\\n",
    "                    'Organization Headquarters', 'Organization Subsidiary Of', 'Organization Subsidiary Of^-1',\\\n",
    "                    'Organization top employees', 'Person Employee or Member of^-1', 'Organization Acquired By^-1',\\\n",
    "                    'Organization Acquired By', 'Organization Provider To', 'Organization Provider To^-1']\n",
    "COMMON_RELATION =  ['Organization Founded By^-1']\n",
    "\n",
    "def add_dummy(df, person=False):\n",
    "    global isPerson\n",
    "    rel = set(df['Relationship'])\n",
    "    if person:\n",
    "        isPerson = person\n",
    "    else:\n",
    "        isPerson = True if any(s in PERSON_RELATIONS for s in rel) else False\n",
    "    if isPerson:\n",
    "        dummy_rels = COMMON_RELATION + PERSON_RELATIONS\n",
    "    else:\n",
    "        dummy_rels = COMMON_RELATION + ORG_RELATION\n",
    "    for r in rel:\n",
    "        dummy_rels.remove(r)\n",
    "    for r in dummy_rels:\n",
    "        df = df.append({'Subject': query, 'Relationship':r, 'Object':''}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def count_confidence(main_df):\n",
    "    if (not main_df.empty):\n",
    "        main_df = main_df.sort_values('Object', ascending=True).drop_duplicates().groupby(['Subject','Relationship']).agg(lambda x: list(x))\n",
    "        main_df['Object'] = main_df['Object'].agg(lambda x: x if x != [''] else [])\n",
    "        main_df['Count'] = main_df['Object'].apply(lambda x: len(x))\n",
    "        #main_df = main_df[[c for c in main_df if c not in ['Confidence']] + ['Confidence']]\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-01T00:45:24.329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline_dict saved!\n",
      "No file exists. Creating a new one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\bs4\\element.py:262: ResourceWarning: unclosed <socket.socket fd=2224, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 64603), raddr=('127.0.0.1', 64594)>\n",
      "  self.parent = parent\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\socket.py:657: ResourceWarning: unclosed <socket.socket fd=7212, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.100.52', 64692), raddr=('151.101.113.171', 443)>\n",
      "  self._sock = None\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\socket.py:657: ResourceWarning: unclosed <socket.socket fd=7828, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.100.52', 64715), raddr=('151.101.113.171', 443)>\n",
      "  self._sock = None\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\socket.py:657: ResourceWarning: unclosed <socket.socket fd=7960, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.100.52', 64720), raddr=('151.101.13.184', 443)>\n",
      "  self._sock = None\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\socket.py:657: ResourceWarning: unclosed <socket.socket fd=4476, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('192.168.100.52', 64723), raddr=('151.101.113.171', 443)>\n",
      "  self._sock = None\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\socket.py:657: ResourceWarning: unclosed <socket.socket fd=8584, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=0, laddr=('2003:e6:9718:e700:bca3:35a8:94aa:464d', 64776, 0, 0), raddr=('2a04:4e42:1b::323', 443, 0, 0)>\n",
      "  self._sock = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n",
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:408: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    \n",
    "    # Scraping News Links & Headlines\n",
    "    make_headline_dict(query)\n",
    "    headline_dict = load_headline_dict(query)\n",
    "    \n",
    "    # Shuffling and taking atleast 100 articles with some extractions\n",
    "    shuffled_list = random.sample(headline_dict.items(), len(headline_dict.items()))\n",
    "    useful100 = load_useful100(query)\n",
    "    lock = Lock()\n",
    "    DFs = []\n",
    "    start = 0\n",
    "\n",
    "    ### Creating threads for each link\n",
    "    while len(useful100.items()) < N :\n",
    "        pass100 = dict(shuffled_list[start:start+N])\n",
    "        threads = []\n",
    "        for headline,link in pass100.items():\n",
    "            threads.append( News(link=link, name=query, lock=lock, headline=headline) )\n",
    "\n",
    "        ### Waiting for each thread to complete\n",
    "        for t in threads:\n",
    "            try:\n",
    "                t.join()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        ### Collecting the dfs\n",
    "        for i,t in enumerate(threads):\n",
    "            headline, link, df = t.get_main_df()\n",
    "            if not df.empty:\n",
    "                DFs.append(df)\n",
    "                useful100[headline] = link\n",
    "\n",
    "        start = min(start+N, len(headline_dict.items()))\n",
    "\n",
    "    save_useful100(useful100)\n",
    "    MERGED_DF = pd.concat(DFs, ignore_index=True)\n",
    "    MERGED_DF.loc[MERGED_DF['Confidence'] == '?','Confidence'] = MERGED_DF['Confidence'].apply(lambda x: round(np.random.uniform(0.85,1),2))\n",
    "    \n",
    "    # Finding Web and Reality\n",
    "    isPerson = False\n",
    "    main_df = MERGED_DF.drop('Confidence',axis=1)\n",
    "    main_df = main_df.groupby(['Subject','Relationship','Object']).size().to_frame('c').reset_index()\n",
    "    main_df = pd.merge(main_df, MERGED_DF)\n",
    "    main_df.Confidence = main_df.Confidence.astype(float).fillna(0.0)\n",
    "    main_df['Object'] = main_df.apply(lambda main_df: main_df['Object']+':'+str(main_df['c']), axis=1) \n",
    "    main_df = main_df.drop('c',axis=1)\n",
    "    main_df = add_dummy(main_df)\n",
    "    main_df = count_confidence(main_df)\n",
    "\n",
    "    ###### ADDING GROUND TRUTH ######\n",
    "    main_df = u.add_ground_truth(main_df)\n",
    "\n",
    "    ###### ADDING RECALL SCORE ######\n",
    "    main_df = u.add_recall_score(main_df)\n",
    "\n",
    "    main_df.to_pickle(\"data/dumps/web_reality-{}-{}.pkl\".format(N,query))\n",
    "    \n",
    "    print(\"Done for {}\".format(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
