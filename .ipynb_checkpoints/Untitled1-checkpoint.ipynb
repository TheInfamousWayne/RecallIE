{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:54:28.040020Z",
     "start_time": "2019-04-01T01:54:21.154849Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from random import randint\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:54:28.249462Z",
     "start_time": "2019-04-01T01:54:28.040020Z"
    }
   },
   "outputs": [],
   "source": [
    "from news.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:54:28.949821Z",
     "start_time": "2019-04-01T01:54:28.250429Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "u = Utils()\n",
    "queries = ['Bill Gates', 'Angela Merkel', 'Donald Trump', 'Ivanka']#, 'Brad Pitt', 'Christopher Nolan', 'Christian Bale', 'Megan Fox', 'Steve Jobs', 'Hillary Clinton', 'Gerhard Weikum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T02:02:15.250712Z",
     "start_time": "2019-04-01T02:02:15.226776Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_headline_dict(query):\n",
    "    ### Getting Custom Date Range ###\n",
    "    y1,m1,d1 = ('2014','01','01')\n",
    "    y2,m2,d2 = str(date.today()).split('-')\n",
    "    \n",
    "    ### Setting up API ###\n",
    "    headline_list = []\n",
    "    headline_dict = {}\n",
    "\n",
    "    options = Options()\n",
    "    options.binary_location = \"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--window-size=1200x600')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "\n",
    "    chromedriver = 'C:\\\\Users\\\\Bhavya\\\\Desktop\\\\Vaibhav\\\\chromedriver.exe'\n",
    "    r = webdriver.Chrome(executable_path=chromedriver, options=options)\n",
    "\n",
    "    ### Scraping From Google ###\n",
    "    final_date = date(int(y2),int(m2),int(d2))\n",
    "    while True:\n",
    "        dd = int(d1)\n",
    "        mm = int(m1) \n",
    "        yy = int(y1) + mm//12\n",
    "        mm = mm % 12 + 1    \n",
    "\n",
    "        temp_date = date(yy,mm,dd)   \n",
    "        if temp_date > final_date:\n",
    "            break  \n",
    "\n",
    "        r.get(\"https://www.google.com/search?q={}&hl=en-US&gl=US&source=lnt&tbs=cdr%3A1%2Ccd_min%3A{}%2F{}%2F{}%2Ccd_max%3A{}%2F{}%2F{}&tbm=nws\".format(\\\n",
    "                            query,m1,d1,y1,mm,dd,yy))\n",
    "        soup = BeautifulSoup(r.page_source, 'lxml')\n",
    "        all_headlines = soup.findAll(\"h3\")\n",
    "        for headline in all_headlines:\n",
    "            headline_list.append(headline.text)\n",
    "            headline_dict[headline.text] = headline.a['href']        \n",
    "        d1,m1,y1 = dd,mm,yy\n",
    "    \n",
    "    ### Saving the file ###\n",
    "    save_headline_dict(headline_dict)\n",
    "    \n",
    "    return headline_dict\n",
    "\n",
    "def save_headline_dict(headline_dict):\n",
    "    if len(headline_dict) > 0:\n",
    "        path = 'data/dumps/{}_headline_dict.pkl'.format(query)\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(headline_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"headline_dict saved!\")\n",
    "        \n",
    "def load_headline_dict(query):\n",
    "    path = 'data/dumps/{}_headline_dict.pkl'.format(query)\n",
    "    headline_dict = {}\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            headline_dict = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"Creating a new Dictionary\")\n",
    "        headline_dict = make_headline_dict(query)\n",
    "    return headline_dict\n",
    "\n",
    "def save_useful100(useful100):\n",
    "    path = 'data/dumps/{}_useful100.pkl'.format(query)\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(useful100, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"100 useful headlines for query {} is saved!\".format(query))\n",
    "        \n",
    "def load_useful100(query):\n",
    "    path = 'data/dumps/{}_useful100.pkl'.format(query)\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            useful100 = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"No file exists. Creating a new one for {}\".format(query))\n",
    "        useful100 = {}\n",
    "    return useful100\n",
    "\n",
    "PERSON_RELATIONS = ['Educated at', 'Citizen of', 'Person Employee or Member of', 'Organization top employees^-1',\\\n",
    "                    'Person Current and Past Location of Residence', 'Person Parents', 'Person Parents^-1',\\\n",
    "                    'Person Place of Birth', 'Person Siblings', 'Person Spouse']\n",
    "ORG_RELATION =     ['Organization Founded By', 'Organization Collaboration', 'Organization Collaboration^-1',\\\n",
    "                    'Organization Headquarters', 'Organization Subsidiary Of', 'Organization Subsidiary Of^-1',\\\n",
    "                    'Organization top employees', 'Person Employee or Member of^-1', 'Organization Acquired By^-1',\\\n",
    "                    'Organization Acquired By', 'Organization Provider To', 'Organization Provider To^-1']\n",
    "COMMON_RELATION =  ['Organization Founded By^-1']\n",
    "\n",
    "def add_dummy(df, person=False):\n",
    "    global isPerson\n",
    "    rel = set(df['Relationship'])\n",
    "    if person:\n",
    "        isPerson = person\n",
    "    else:\n",
    "        isPerson = True if any(s in PERSON_RELATIONS for s in rel) else False\n",
    "    if isPerson:\n",
    "        dummy_rels = COMMON_RELATION + PERSON_RELATIONS\n",
    "    else:\n",
    "        dummy_rels = COMMON_RELATION + ORG_RELATION\n",
    "    for r in rel:\n",
    "        dummy_rels.remove(r)\n",
    "    for r in dummy_rels:\n",
    "        df = df.append({'Subject': query, 'Relationship':r, 'Object':''}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def count_confidence(main_df):\n",
    "    if (not main_df.empty):\n",
    "        main_df = main_df.sort_values('Object', ascending=True).drop_duplicates().groupby(['Subject','Relationship']).agg(lambda x: list(x))\n",
    "        main_df['Object'] = main_df['Object'].agg(lambda x: x if x != [''] else [])\n",
    "        main_df['Count'] = main_df['Object'].apply(lambda x: len(x))\n",
    "        #main_df = main_df[[c for c in main_df if c not in ['Confidence']] + ['Confidence']]\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T01:59:56.229902Z",
     "start_time": "2019-04-01T01:59:03.561292Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new Dictionary\n",
      "0\n",
      "No file exists. Creating a new one for Bill Gates\n",
      "0 100  ----  0 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0800a64ac6e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data/dumps/web_reality-{}-{}.pkl\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Already exists for {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/dumps/web_reality-100-Bill Gates.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0800a64ac6e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0msave_useful100\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0museful100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mMERGED_DF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDFs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mMERGED_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMERGED_DF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Confidence'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'?'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Confidence'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMERGED_DF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Confidence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.85\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Did Confidence\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    226\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    229\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No objects to concatenate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    \n",
    "    # Scraping News Links & Headlines\n",
    "    headline_dict = load_headline_dict(query)\n",
    "    print(len(headline_dict))\n",
    "    \n",
    "    # Shuffling and taking atleast 100 articles with some extractions\n",
    "    shuffled_list = random.sample(headline_dict.items(), len(headline_dict.items()))\n",
    "    useful100 = load_useful100(query)\n",
    "    lock = Lock()\n",
    "    DFs = []\n",
    "    start = 0\n",
    "    \n",
    "    if len(useful100.items()) >= 100:\n",
    "        shuffled_list = random.sample(useful100.items(), len(useful100.items()))\n",
    "        \n",
    "    try:\n",
    "        path = \"data/dumps/web_reality-{}-{}.pkl\".format(N,query)\n",
    "        fh = open(path, 'rb')\n",
    "        print(\"Already exists for {}\".format(query))\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(len(DFs),N, \" ---- \",start, len(shuffled_list))\n",
    "        ### Creating threads for each link\n",
    "        while len(DFs) < N  and start < len(shuffled_list):\n",
    "            pass100 = dict(shuffled_list[start:start+N])\n",
    "            threads = []\n",
    "            for headline,link in pass100.items():\n",
    "                threads.append( News(link=link, name=query, lock=lock, headline=headline) )\n",
    "\n",
    "            ### Waiting for each thread to complete\n",
    "            for t in threads:\n",
    "                try:\n",
    "                    t.join()\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "            ### Collecting the dfs\n",
    "            for i,t in enumerate(threads):\n",
    "                headline, link, df = t.get_main_df()\n",
    "                if not df.empty:\n",
    "                    DFs.append(df)\n",
    "                    useful100[headline] = link\n",
    "\n",
    "            start = min(start+N, len(headline_dict.items()))\n",
    "            print(start)\n",
    "        \n",
    "        if len(useful100) > 0:\n",
    "            save_useful100(useful100)\n",
    "        \n",
    "        MERGED_DF = pd.concat(DFs, ignore_index=True)\n",
    "        MERGED_DF.loc[MERGED_DF['Confidence'] == '?','Confidence'] = MERGED_DF['Confidence'].apply(lambda x: round(np.random.uniform(0.85,1),2))\n",
    "        print(\"Did Confidence\")\n",
    "\n",
    "        # Finding Web and Reality\n",
    "        isPerson = False\n",
    "        main_df = MERGED_DF.drop('Confidence',axis=1)\n",
    "        main_df = main_df.groupby(['Subject','Relationship','Object']).size().to_frame('c').reset_index()\n",
    "        main_df = pd.merge(main_df, MERGED_DF)\n",
    "        main_df.Confidence = main_df.Confidence.astype(float).fillna(0.0)\n",
    "        main_df['Object'] = main_df.apply(lambda main_df: main_df['Object']+':'+str(main_df['c']), axis=1) \n",
    "        main_df = main_df.drop('c',axis=1)\n",
    "        main_df = add_dummy(main_df)\n",
    "        main_df = count_confidence(main_df)\n",
    "        print(\"Did merge\")\n",
    "\n",
    "        ###### ADDING GROUND TRUTH ######\n",
    "        main_df = u.add_ground_truth(main_df)\n",
    "\n",
    "        ###### ADDING RECALL SCORE ######\n",
    "        main_df = u.add_recall_score(main_df)\n",
    "\n",
    "        main_df.to_pickle(\"data/dumps/web_reality-{}-{}.pkl\".format(N,query))\n",
    "\n",
    "        print(\"Done for {}\".format(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T02:02:09.433040Z",
     "start_time": "2019-04-01T02:01:17.689556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new Dictionary\n"
     ]
    }
   ],
   "source": [
    "headline_dict = load_headline_dict(\"Bill Gates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T02:02:09.440937Z",
     "start_time": "2019-04-01T02:02:09.435989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
