{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:57:49.504755Z",
     "start_time": "2019-04-01T00:57:49.497791Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from random import randint\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:57:50.020761Z",
     "start_time": "2019-04-01T00:57:50.015811Z"
    }
   },
   "outputs": [],
   "source": [
    "from news.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:57:52.571858Z",
     "start_time": "2019-04-01T00:57:52.564880Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "u = Utils()\n",
    "queries = ['Angela Merkel', 'Donald Trump', 'Ivanka', 'Bill Gates']#, 'Brad Pitt', 'Christopher Nolan', 'Christian Bale', 'Megan Fox', 'Steve Jobs', 'Hillary Clinton', 'Gerhard Weikum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T00:57:55.334333Z",
     "start_time": "2019-04-01T00:57:55.305412Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_headline_dict(query):\n",
    "    ### Getting Custom Date Range ###\n",
    "    y1,m1,d1 = ('2014','01','01')\n",
    "    y2,m2,d2 = str(date.today()).split('-')\n",
    "    \n",
    "    ### Setting up API ###\n",
    "    headline_list = []\n",
    "    headline_dict = {}\n",
    "\n",
    "    options = Options()\n",
    "    options.binary_location = \"C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe\"\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--window-size=1200x600')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "\n",
    "    chromedriver = 'C:\\\\Users\\\\Bhavya\\\\Desktop\\\\Vaibhav\\\\chromedriver.exe'\n",
    "    r = webdriver.Chrome(executable_path=chromedriver, options=options)\n",
    "\n",
    "    ### Scraping From Google ###\n",
    "    final_date = date(int(y2),int(m2),int(d2))\n",
    "    while True:\n",
    "        dd = int(d1)\n",
    "        mm = int(m1) \n",
    "        yy = int(y1) + mm//12\n",
    "        mm = mm % 12 + 1    \n",
    "\n",
    "        temp_date = date(yy,mm,dd)   \n",
    "        if temp_date > final_date:\n",
    "            break  \n",
    "\n",
    "        r.get(\"https://www.google.com/search?q={}&hl=en-US&gl=US&source=lnt&tbs=cdr%3A1%2Ccd_min%3A{}%2F{}%2F{}%2Ccd_max%3A{}%2F{}%2F{}&tbm=nws\".format(\\\n",
    "                            query,m1,d1,y1,mm,dd,yy))\n",
    "        soup = BeautifulSoup(r.page_source, 'lxml')\n",
    "        all_headlines = soup.findAll(\"h3\")\n",
    "        for headline in all_headlines:\n",
    "            headline_list.append(headline.text)\n",
    "            headline_dict[headline.text] = headline.a['href']        \n",
    "        d1,m1,y1 = dd,mm,yy\n",
    "    \n",
    "    ### Saving the file ###\n",
    "    save_headline_dict(headline_dict)\n",
    "\n",
    "def save_headline_dict(headline_dict):\n",
    "    path = 'data/dumps/{}_headline_dict.pkl'.format(query)\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(headline_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"headline_dict saved!\")\n",
    "        \n",
    "def load_headline_dict(query):\n",
    "    path = 'data/dumps/{}_headline_dict.pkl'.format(query)\n",
    "    headline_dict = {}\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            headline_dict = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"Creating a new Dictionary\")\n",
    "        headline_dict = {}\n",
    "    return headline_dict\n",
    "\n",
    "def save_useful100(useful100):\n",
    "    path = 'data/dumps/{}_useful100.pkl'.format(query)\n",
    "    with open(path, 'wb') as fp:\n",
    "        pickle.dump(useful100, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"100 useful headlines for query {} is saved!\".format(query))\n",
    "        \n",
    "def load_useful100(query):\n",
    "    path = 'data/dumps/{}_useful100.pkl'.format(query)\n",
    "    try:\n",
    "        with open(path, 'rb') as fp:\n",
    "            useful100 = pickle.load(fp)\n",
    "    except:\n",
    "        print (\"No file exists. Creating a new one for {}\".format(query))\n",
    "        useful100 = {}\n",
    "    return useful100\n",
    "\n",
    "PERSON_RELATIONS = ['Educated at', 'Citizen of', 'Person Employee or Member of', 'Organization top employees^-1',\\\n",
    "                    'Person Current and Past Location of Residence', 'Person Parents', 'Person Parents^-1',\\\n",
    "                    'Person Place of Birth', 'Person Siblings', 'Person Spouse']\n",
    "ORG_RELATION =     ['Organization Founded By', 'Organization Collaboration', 'Organization Collaboration^-1',\\\n",
    "                    'Organization Headquarters', 'Organization Subsidiary Of', 'Organization Subsidiary Of^-1',\\\n",
    "                    'Organization top employees', 'Person Employee or Member of^-1', 'Organization Acquired By^-1',\\\n",
    "                    'Organization Acquired By', 'Organization Provider To', 'Organization Provider To^-1']\n",
    "COMMON_RELATION =  ['Organization Founded By^-1']\n",
    "\n",
    "def add_dummy(df, person=False):\n",
    "    global isPerson\n",
    "    rel = set(df['Relationship'])\n",
    "    if person:\n",
    "        isPerson = person\n",
    "    else:\n",
    "        isPerson = True if any(s in PERSON_RELATIONS for s in rel) else False\n",
    "    if isPerson:\n",
    "        dummy_rels = COMMON_RELATION + PERSON_RELATIONS\n",
    "    else:\n",
    "        dummy_rels = COMMON_RELATION + ORG_RELATION\n",
    "    for r in rel:\n",
    "        dummy_rels.remove(r)\n",
    "    for r in dummy_rels:\n",
    "        df = df.append({'Subject': query, 'Relationship':r, 'Object':''}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def count_confidence(main_df):\n",
    "    if (not main_df.empty):\n",
    "        main_df = main_df.sort_values('Object', ascending=True).drop_duplicates().groupby(['Subject','Relationship']).agg(lambda x: list(x))\n",
    "        main_df['Object'] = main_df['Object'].agg(lambda x: x if x != [''] else [])\n",
    "        main_df['Count'] = main_df['Object'].apply(lambda x: len(x))\n",
    "        #main_df = main_df[[c for c in main_df if c not in ['Confidence']] + ['Confidence']]\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-01T00:57:58.273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline_dict saved!\n",
      "No file exists. Creating a new one for Angela Merkel\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    \n",
    "    # Scraping News Links & Headlines\n",
    "    make_headline_dict(query)\n",
    "    headline_dict = load_headline_dict(query)\n",
    "    \n",
    "    # Shuffling and taking atleast 100 articles with some extractions\n",
    "    shuffled_list = random.sample(headline_dict.items(), len(headline_dict.items()))\n",
    "    useful100 = load_useful100(query)\n",
    "    lock = Lock()\n",
    "    DFs = []\n",
    "    start = 0\n",
    "\n",
    "    ### Creating threads for each link\n",
    "    while len(useful100.items()) < N :\n",
    "        pass100 = dict(shuffled_list[start:start+N])\n",
    "        threads = []\n",
    "        for headline,link in pass100.items():\n",
    "            threads.append( News(link=link, name=query, lock=lock, headline=headline) )\n",
    "\n",
    "        ### Waiting for each thread to complete\n",
    "        for t in threads:\n",
    "            try:\n",
    "                t.join()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        ### Collecting the dfs\n",
    "        for i,t in enumerate(threads):\n",
    "            headline, link, df = t.get_main_df()\n",
    "            if not df.empty:\n",
    "                DFs.append(df)\n",
    "                useful100[headline] = link\n",
    "\n",
    "        start = min(start+N, len(headline_dict.items()))\n",
    "\n",
    "    save_useful100(useful100)\n",
    "    MERGED_DF = pd.concat(DFs, ignore_index=True)\n",
    "    MERGED_DF.loc[MERGED_DF['Confidence'] == '?','Confidence'] = MERGED_DF['Confidence'].apply(lambda x: round(np.random.uniform(0.85,1),2))\n",
    "    \n",
    "    # Finding Web and Reality\n",
    "    isPerson = False\n",
    "    main_df = MERGED_DF.drop('Confidence',axis=1)\n",
    "    main_df = main_df.groupby(['Subject','Relationship','Object']).size().to_frame('c').reset_index()\n",
    "    main_df = pd.merge(main_df, MERGED_DF)\n",
    "    main_df.Confidence = main_df.Confidence.astype(float).fillna(0.0)\n",
    "    main_df['Object'] = main_df.apply(lambda main_df: main_df['Object']+':'+str(main_df['c']), axis=1) \n",
    "    main_df = main_df.drop('c',axis=1)\n",
    "    main_df = add_dummy(main_df)\n",
    "    main_df = count_confidence(main_df)\n",
    "\n",
    "    ###### ADDING GROUND TRUTH ######\n",
    "    main_df = u.add_ground_truth(main_df)\n",
    "\n",
    "    ###### ADDING RECALL SCORE ######\n",
    "    main_df = u.add_recall_score(main_df)\n",
    "\n",
    "    main_df.to_pickle(\"data/dumps/web_reality-{}-{}.pkl\".format(N,query))\n",
    "    \n",
    "    print(\"Done for {}\".format(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
